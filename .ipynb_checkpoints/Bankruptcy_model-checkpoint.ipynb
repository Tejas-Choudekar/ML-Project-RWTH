{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a1736c",
   "metadata": {},
   "source": [
    "## *Bankruptcy: Machine Learning Project*\n",
    "#### Winter Sem: 2021-22,\n",
    "| Submitted By: | Matriculation Number |\n",
    "| --- | --- |\n",
    "| Ajit Kumar Gupta | 429395 |\n",
    "| Karthik Ananthakrishnan | 428859 |\n",
    "| Cristian Castro | 42732 |\n",
    "| Tejas Choudekar | 428841 |\n",
    "| Ana Aragones| 433641 |\n",
    "| Bo Hu | 415101 |\n",
    "\n",
    "\n",
    "In this project we,\n",
    "1. Perform data exploration, cleaning, imputation.\n",
    "2. Use data visualisation techniques to explore the data set.\n",
    "3. Come up with (binary) classification models to predict whether a company will be bankrupt.\n",
    "4. Perform model selection among these models, with a rigorous methodology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67f7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfc9ad",
   "metadata": {},
   "source": [
    "## *1. Importing the Dataset and explore the features, missing values and outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6796ab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>Attr11</th>\n",
       "      <th>Attr12</th>\n",
       "      <th>Attr13</th>\n",
       "      <th>Attr14</th>\n",
       "      <th>Attr15</th>\n",
       "      <th>Attr16</th>\n",
       "      <th>Attr17</th>\n",
       "      <th>Attr18</th>\n",
       "      <th>Attr19</th>\n",
       "      <th>Attr20</th>\n",
       "      <th>Attr21</th>\n",
       "      <th>Attr22</th>\n",
       "      <th>Attr23</th>\n",
       "      <th>Attr24</th>\n",
       "      <th>Attr25</th>\n",
       "      <th>Attr26</th>\n",
       "      <th>Attr27</th>\n",
       "      <th>Attr28</th>\n",
       "      <th>Attr29</th>\n",
       "      <th>Attr30</th>\n",
       "      <th>Attr31</th>\n",
       "      <th>Attr32</th>\n",
       "      <th>Attr33</th>\n",
       "      <th>Attr34</th>\n",
       "      <th>Attr35</th>\n",
       "      <th>Attr36</th>\n",
       "      <th>Attr37</th>\n",
       "      <th>Attr38</th>\n",
       "      <th>Attr39</th>\n",
       "      <th>Attr40</th>\n",
       "      <th>Attr41</th>\n",
       "      <th>Attr42</th>\n",
       "      <th>Attr43</th>\n",
       "      <th>Attr44</th>\n",
       "      <th>Attr45</th>\n",
       "      <th>Attr46</th>\n",
       "      <th>Attr47</th>\n",
       "      <th>Attr48</th>\n",
       "      <th>Attr49</th>\n",
       "      <th>Attr50</th>\n",
       "      <th>Attr51</th>\n",
       "      <th>Attr52</th>\n",
       "      <th>Attr53</th>\n",
       "      <th>Attr54</th>\n",
       "      <th>Attr55</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174190</td>\n",
       "      <td>0.41299</td>\n",
       "      <td>0.14371</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>-28.9820</td>\n",
       "      <td>0.60383</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>1.1225</td>\n",
       "      <td>1.1961</td>\n",
       "      <td>0.46359</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>0.531390</td>\n",
       "      <td>0.142330</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>592.24</td>\n",
       "      <td>0.61630</td>\n",
       "      <td>2.4213</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>37.573</td>\n",
       "      <td>0.99690</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>0.097402</td>\n",
       "      <td>0.75641</td>\n",
       "      <td>0.46359</td>\n",
       "      <td>0.50669</td>\n",
       "      <td>1.97370</td>\n",
       "      <td>0.32417</td>\n",
       "      <td>5.9473</td>\n",
       "      <td>0.224930</td>\n",
       "      <td>0.122720</td>\n",
       "      <td>100.820</td>\n",
       "      <td>3.6203</td>\n",
       "      <td>0.71453</td>\n",
       "      <td>0.295100</td>\n",
       "      <td>1.8079</td>\n",
       "      <td>123140.0000</td>\n",
       "      <td>0.46359</td>\n",
       "      <td>0.165010</td>\n",
       "      <td>0.212820</td>\n",
       "      <td>0.041124</td>\n",
       "      <td>0.165010</td>\n",
       "      <td>95.682</td>\n",
       "      <td>58.109</td>\n",
       "      <td>0.946210</td>\n",
       "      <td>0.90221</td>\n",
       "      <td>44.941</td>\n",
       "      <td>0.260030</td>\n",
       "      <td>0.145400</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>0.41299</td>\n",
       "      <td>0.27622</td>\n",
       "      <td>1.0457</td>\n",
       "      <td>1.0458</td>\n",
       "      <td>127280.0</td>\n",
       "      <td>0.163960</td>\n",
       "      <td>0.375740</td>\n",
       "      <td>0.83604</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.7145</td>\n",
       "      <td>6.2813</td>\n",
       "      <td>84.291</td>\n",
       "      <td>4.3303</td>\n",
       "      <td>4.0341</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.46038</td>\n",
       "      <td>0.28230</td>\n",
       "      <td>1.6294</td>\n",
       "      <td>2.5952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>1.1721</td>\n",
       "      <td>1.6018</td>\n",
       "      <td>0.53962</td>\n",
       "      <td>0.175790</td>\n",
       "      <td>0.383170</td>\n",
       "      <td>0.126470</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>829.46</td>\n",
       "      <td>0.44004</td>\n",
       "      <td>2.1721</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>0.107280</td>\n",
       "      <td>60.954</td>\n",
       "      <td>5.08890</td>\n",
       "      <td>0.175710</td>\n",
       "      <td>0.091295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17523</td>\n",
       "      <td>0.38442</td>\n",
       "      <td>44.59300</td>\n",
       "      <td>1.04860</td>\n",
       "      <td>4.0792</td>\n",
       "      <td>0.243840</td>\n",
       "      <td>0.109740</td>\n",
       "      <td>105.090</td>\n",
       "      <td>3.4733</td>\n",
       "      <td>3.38360</td>\n",
       "      <td>0.044076</td>\n",
       "      <td>1.6018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.53962</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.164060</td>\n",
       "      <td>0.074333</td>\n",
       "      <td>0.109690</td>\n",
       "      <td>149.750</td>\n",
       "      <td>88.801</td>\n",
       "      <td>0.546690</td>\n",
       "      <td>1.03300</td>\n",
       "      <td>62.678</td>\n",
       "      <td>0.144970</td>\n",
       "      <td>0.090503</td>\n",
       "      <td>1.5874</td>\n",
       "      <td>0.44849</td>\n",
       "      <td>0.28791</td>\n",
       "      <td>2.0044</td>\n",
       "      <td>2.0044</td>\n",
       "      <td>3387.8</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.90108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.9882</td>\n",
       "      <td>4.1103</td>\n",
       "      <td>102.190</td>\n",
       "      <td>3.5716</td>\n",
       "      <td>5.9500</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.22612</td>\n",
       "      <td>0.48839</td>\n",
       "      <td>3.1599</td>\n",
       "      <td>84.8740</td>\n",
       "      <td>0.19114</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>2.9881</td>\n",
       "      <td>1.0077</td>\n",
       "      <td>0.67566</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>2094.10</td>\n",
       "      <td>0.17430</td>\n",
       "      <td>4.4225</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>53.881</td>\n",
       "      <td>0.67451</td>\n",
       "      <td>0.040610</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.23470</td>\n",
       "      <td>0.67566</td>\n",
       "      <td>0.15672</td>\n",
       "      <td>0.32153</td>\n",
       "      <td>1.71070</td>\n",
       "      <td>4.6220</td>\n",
       "      <td>0.036196</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>65.345</td>\n",
       "      <td>5.5857</td>\n",
       "      <td>0.17960</td>\n",
       "      <td>0.040610</td>\n",
       "      <td>1.3425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.67566</td>\n",
       "      <td>0.031907</td>\n",
       "      <td>0.844690</td>\n",
       "      <td>0.098528</td>\n",
       "      <td>0.031907</td>\n",
       "      <td>150.130</td>\n",
       "      <td>96.251</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>2.32900</td>\n",
       "      <td>54.296</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.004533</td>\n",
       "      <td>3.1599</td>\n",
       "      <td>0.22612</td>\n",
       "      <td>0.17903</td>\n",
       "      <td>2.3667</td>\n",
       "      <td>2.3667</td>\n",
       "      <td>20453.0</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.99236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.7742</td>\n",
       "      <td>3.7922</td>\n",
       "      <td>64.846</td>\n",
       "      <td>5.6287</td>\n",
       "      <td>4.4581</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.43236</td>\n",
       "      <td>0.27546</td>\n",
       "      <td>1.7833</td>\n",
       "      <td>-10.1050</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>1.3057</td>\n",
       "      <td>1.0509</td>\n",
       "      <td>0.56453</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.069747</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>3299.40</td>\n",
       "      <td>0.11063</td>\n",
       "      <td>2.3129</td>\n",
       "      <td>0.024526</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>86.317</td>\n",
       "      <td>0.62795</td>\n",
       "      <td>0.055446</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>0.56944</td>\n",
       "      <td>0.56453</td>\n",
       "      <td>0.11063</td>\n",
       "      <td>0.44844</td>\n",
       "      <td>0.73869</td>\n",
       "      <td>4.2600</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>103.810</td>\n",
       "      <td>3.5161</td>\n",
       "      <td>0.12824</td>\n",
       "      <td>0.055446</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>3.9624</td>\n",
       "      <td>0.64524</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>0.178260</td>\n",
       "      <td>0.180500</td>\n",
       "      <td>0.042673</td>\n",
       "      <td>158.550</td>\n",
       "      <td>72.237</td>\n",
       "      <td>0.079819</td>\n",
       "      <td>0.90954</td>\n",
       "      <td>90.707</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.024737</td>\n",
       "      <td>1.4504</td>\n",
       "      <td>0.35164</td>\n",
       "      <td>0.28440</td>\n",
       "      <td>1.5139</td>\n",
       "      <td>1.7303</td>\n",
       "      <td>5012.6</td>\n",
       "      <td>0.048398</td>\n",
       "      <td>0.043445</td>\n",
       "      <td>0.95160</td>\n",
       "      <td>0.142980</td>\n",
       "      <td>4.2286</td>\n",
       "      <td>5.0528</td>\n",
       "      <td>98.783</td>\n",
       "      <td>3.6950</td>\n",
       "      <td>3.4844</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.188290</td>\n",
       "      <td>0.41504</td>\n",
       "      <td>0.34231</td>\n",
       "      <td>1.9279</td>\n",
       "      <td>-58.2740</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.233580</td>\n",
       "      <td>1.4094</td>\n",
       "      <td>1.3393</td>\n",
       "      <td>0.58496</td>\n",
       "      <td>0.238810</td>\n",
       "      <td>0.633170</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.233580</td>\n",
       "      <td>602.31</td>\n",
       "      <td>0.60600</td>\n",
       "      <td>2.4094</td>\n",
       "      <td>0.233580</td>\n",
       "      <td>0.174410</td>\n",
       "      <td>140.860</td>\n",
       "      <td>1.20390</td>\n",
       "      <td>0.234930</td>\n",
       "      <td>0.140590</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57250</td>\n",
       "      <td>0.49687</td>\n",
       "      <td>44.94700</td>\n",
       "      <td>1.18530</td>\n",
       "      <td>4.6033</td>\n",
       "      <td>0.306910</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>122.090</td>\n",
       "      <td>2.9897</td>\n",
       "      <td>2.65740</td>\n",
       "      <td>0.236350</td>\n",
       "      <td>1.3393</td>\n",
       "      <td>4.5490</td>\n",
       "      <td>0.62769</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.054712</td>\n",
       "      <td>0.175420</td>\n",
       "      <td>192.450</td>\n",
       "      <td>51.585</td>\n",
       "      <td>0.364290</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>171.050</td>\n",
       "      <td>0.216990</td>\n",
       "      <td>0.162030</td>\n",
       "      <td>1.7136</td>\n",
       "      <td>0.36891</td>\n",
       "      <td>0.33449</td>\n",
       "      <td>2.0256</td>\n",
       "      <td>2.1735</td>\n",
       "      <td>13730.0</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.321880</td>\n",
       "      <td>0.82635</td>\n",
       "      <td>0.073039</td>\n",
       "      <td>2.5912</td>\n",
       "      <td>7.0756</td>\n",
       "      <td>100.540</td>\n",
       "      <td>3.6303</td>\n",
       "      <td>4.6375</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Attr1    Attr2    Attr3   Attr4    Attr5    Attr6     Attr7   Attr8  \\\n",
       "0  0.174190  0.41299  0.14371  1.3480 -28.9820  0.60383  0.219460  1.1225   \n",
       "1  0.146240  0.46038  0.28230  1.6294   2.5952  0.00000  0.171850  1.1721   \n",
       "2  0.000595  0.22612  0.48839  3.1599  84.8740  0.19114  0.004572  2.9881   \n",
       "3  0.024526  0.43236  0.27546  1.7833 -10.1050  0.56944  0.024526  1.3057   \n",
       "4  0.188290  0.41504  0.34231  1.9279 -58.2740  0.00000  0.233580  1.4094   \n",
       "\n",
       "    Attr9   Attr10    Attr11    Attr12    Attr13    Attr14   Attr15   Attr16  \\\n",
       "0  1.1961  0.46359  0.219460  0.531390  0.142330  0.219460   592.24  0.61630   \n",
       "1  1.6018  0.53962  0.175790  0.383170  0.126470  0.171850   829.46  0.44004   \n",
       "2  1.0077  0.67566  0.004572  0.020219  0.030966  0.004572  2094.10  0.17430   \n",
       "3  1.0509  0.56453  0.024526  0.069747  0.036812  0.024526  3299.40  0.11063   \n",
       "4  1.3393  0.58496  0.238810  0.633170  0.187800  0.233580   602.31  0.60600   \n",
       "\n",
       "   Attr17    Attr18    Attr19   Attr20   Attr21    Attr22    Attr23   Attr24  \\\n",
       "0  2.4213  0.219460  0.122720   37.573  0.99690  0.295100  0.097402  0.75641   \n",
       "1  2.1721  0.171850  0.107280   60.954  5.08890  0.175710  0.091295      NaN   \n",
       "2  4.4225  0.004572  0.003592   53.881  0.67451  0.040610  0.000468  0.23470   \n",
       "3  2.3129  0.024526  0.018876   86.317  0.62795  0.055446  0.018876  0.56944   \n",
       "4  2.4094  0.233580  0.174410  140.860  1.20390  0.234930  0.140590  0.00000   \n",
       "\n",
       "    Attr25   Attr26    Attr27   Attr28  Attr29    Attr30    Attr31   Attr32  \\\n",
       "0  0.46359  0.50669   1.97370  0.32417  5.9473  0.224930  0.122720  100.820   \n",
       "1  0.17523  0.38442  44.59300  1.04860  4.0792  0.243840  0.109740  105.090   \n",
       "2  0.67566  0.15672   0.32153  1.71070  4.6220  0.036196  0.003592   65.345   \n",
       "3  0.56453  0.11063   0.44844  0.73869  4.2600  0.286240  0.018876  103.810   \n",
       "4  0.57250  0.49687  44.94700  1.18530  4.6033  0.306910  0.177840  122.090   \n",
       "\n",
       "   Attr33   Attr34    Attr35  Attr36       Attr37   Attr38    Attr39  \\\n",
       "0  3.6203  0.71453  0.295100  1.8079  123140.0000  0.46359  0.165010   \n",
       "1  3.4733  3.38360  0.044076  1.6018          NaN  0.53962  0.027516   \n",
       "2  5.5857  0.17960  0.040610  1.3425          NaN  0.67566  0.031907   \n",
       "3  3.5161  0.12824  0.055446  1.3068       3.9624  0.64524  0.042673   \n",
       "4  2.9897  2.65740  0.236350  1.3393       4.5490  0.62769  0.176480   \n",
       "\n",
       "     Attr40    Attr41    Attr42   Attr43  Attr44    Attr45   Attr46   Attr47  \\\n",
       "0  0.212820  0.041124  0.165010   95.682  58.109  0.946210  0.90221   44.941   \n",
       "1  0.164060  0.074333  0.109690  149.750  88.801  0.546690  1.03300   62.678   \n",
       "2  0.844690  0.098528  0.031907  150.130  96.251  0.003168  2.32900   54.296   \n",
       "3  0.178260  0.180500  0.042673  158.550  72.237  0.079819  0.90954   90.707   \n",
       "4  0.013769  0.054712  0.175420  192.450  51.585  0.364290  0.52685  171.050   \n",
       "\n",
       "     Attr48    Attr49  Attr50   Attr51   Attr52  Attr53  Attr54    Attr55  \\\n",
       "0  0.260030  0.145400  1.3480  0.41299  0.27622  1.0457  1.0458  127280.0   \n",
       "1  0.144970  0.090503  1.5874  0.44849  0.28791  2.0044  2.0044    3387.8   \n",
       "2  0.005769  0.004533  3.1599  0.22612  0.17903  2.3667  2.3667   20453.0   \n",
       "3  0.032141  0.024737  1.4504  0.35164  0.28440  1.5139  1.7303    5012.6   \n",
       "4  0.216990  0.162030  1.7136  0.36891  0.33449  2.0256  2.1735   13730.0   \n",
       "\n",
       "     Attr56    Attr57   Attr58    Attr59  Attr60  Attr61   Attr62  Attr63  \\\n",
       "0  0.163960  0.375740  0.83604  0.000007  9.7145  6.2813   84.291  4.3303   \n",
       "1  0.027516  0.271000  0.90108  0.000000  5.9882  4.1103  102.190  3.5716   \n",
       "2  0.007639  0.000881  0.99236  0.000000  6.7742  3.7922   64.846  5.6287   \n",
       "3  0.048398  0.043445  0.95160  0.142980  4.2286  5.0528   98.783  3.6950   \n",
       "4  0.176480  0.321880  0.82635  0.073039  2.5912  7.0756  100.540  3.6303   \n",
       "\n",
       "   Attr64 class  \n",
       "0  4.0341  b'0'  \n",
       "1  5.9500  b'0'  \n",
       "2  4.4581  b'0'  \n",
       "3  3.4844  b'0'  \n",
       "4  4.6375  b'0'  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows',65)\n",
    "pd.set_option('display.max_columns',65)\n",
    "data = pd.read_csv(\"bankruptcy.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b62178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10503 entries, 0 to 10502\n",
      "Data columns (total 65 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Attr1   10503 non-null  float64\n",
      " 1   Attr2   10503 non-null  float64\n",
      " 2   Attr3   10503 non-null  float64\n",
      " 3   Attr4   10485 non-null  float64\n",
      " 4   Attr5   10478 non-null  float64\n",
      " 5   Attr6   10503 non-null  float64\n",
      " 6   Attr7   10503 non-null  float64\n",
      " 7   Attr8   10489 non-null  float64\n",
      " 8   Attr9   10500 non-null  float64\n",
      " 9   Attr10  10503 non-null  float64\n",
      " 10  Attr11  10503 non-null  float64\n",
      " 11  Attr12  10485 non-null  float64\n",
      " 12  Attr13  10460 non-null  float64\n",
      " 13  Attr14  10503 non-null  float64\n",
      " 14  Attr15  10495 non-null  float64\n",
      " 15  Attr16  10489 non-null  float64\n",
      " 16  Attr17  10489 non-null  float64\n",
      " 17  Attr18  10503 non-null  float64\n",
      " 18  Attr19  10460 non-null  float64\n",
      " 19  Attr20  10460 non-null  float64\n",
      " 20  Attr21  9696 non-null   float64\n",
      " 21  Attr22  10503 non-null  float64\n",
      " 22  Attr23  10460 non-null  float64\n",
      " 23  Attr24  10276 non-null  float64\n",
      " 24  Attr25  10503 non-null  float64\n",
      " 25  Attr26  10489 non-null  float64\n",
      " 26  Attr27  9788 non-null   float64\n",
      " 27  Attr28  10275 non-null  float64\n",
      " 28  Attr29  10503 non-null  float64\n",
      " 29  Attr30  10460 non-null  float64\n",
      " 30  Attr31  10460 non-null  float64\n",
      " 31  Attr32  10402 non-null  float64\n",
      " 32  Attr33  10485 non-null  float64\n",
      " 33  Attr34  10489 non-null  float64\n",
      " 34  Attr35  10503 non-null  float64\n",
      " 35  Attr36  10503 non-null  float64\n",
      " 36  Attr37  5767 non-null   float64\n",
      " 37  Attr38  10503 non-null  float64\n",
      " 38  Attr39  10460 non-null  float64\n",
      " 39  Attr40  10485 non-null  float64\n",
      " 40  Attr41  10301 non-null  float64\n",
      " 41  Attr42  10460 non-null  float64\n",
      " 42  Attr43  10460 non-null  float64\n",
      " 43  Attr44  10460 non-null  float64\n",
      " 44  Attr45  9912 non-null   float64\n",
      " 45  Attr46  10485 non-null  float64\n",
      " 46  Attr47  10417 non-null  float64\n",
      " 47  Attr48  10503 non-null  float64\n",
      " 48  Attr49  10460 non-null  float64\n",
      " 49  Attr50  10489 non-null  float64\n",
      " 50  Attr51  10503 non-null  float64\n",
      " 51  Attr52  10417 non-null  float64\n",
      " 52  Attr53  10275 non-null  float64\n",
      " 53  Attr54  10275 non-null  float64\n",
      " 54  Attr55  10503 non-null  float64\n",
      " 55  Attr56  10460 non-null  float64\n",
      " 56  Attr57  10503 non-null  float64\n",
      " 57  Attr58  10474 non-null  float64\n",
      " 58  Attr59  10503 non-null  float64\n",
      " 59  Attr60  9911 non-null   float64\n",
      " 60  Attr61  10486 non-null  float64\n",
      " 61  Attr62  10460 non-null  float64\n",
      " 62  Attr63  10485 non-null  float64\n",
      " 63  Attr64  10275 non-null  float64\n",
      " 64  class   10503 non-null  object \n",
      "dtypes: float64(64), object(1)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d06797",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{All our features are numerical and the label is Categorical, b'0': Non-Bankrupt companies, b'1':Bankrupt companies}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e34bb89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>Attr11</th>\n",
       "      <th>Attr12</th>\n",
       "      <th>Attr13</th>\n",
       "      <th>Attr14</th>\n",
       "      <th>Attr15</th>\n",
       "      <th>Attr16</th>\n",
       "      <th>Attr17</th>\n",
       "      <th>Attr18</th>\n",
       "      <th>Attr19</th>\n",
       "      <th>Attr20</th>\n",
       "      <th>Attr21</th>\n",
       "      <th>Attr22</th>\n",
       "      <th>Attr23</th>\n",
       "      <th>Attr24</th>\n",
       "      <th>Attr25</th>\n",
       "      <th>Attr26</th>\n",
       "      <th>Attr27</th>\n",
       "      <th>Attr28</th>\n",
       "      <th>Attr29</th>\n",
       "      <th>Attr30</th>\n",
       "      <th>Attr31</th>\n",
       "      <th>Attr32</th>\n",
       "      <th>Attr33</th>\n",
       "      <th>Attr34</th>\n",
       "      <th>Attr35</th>\n",
       "      <th>Attr36</th>\n",
       "      <th>Attr37</th>\n",
       "      <th>Attr38</th>\n",
       "      <th>Attr39</th>\n",
       "      <th>Attr40</th>\n",
       "      <th>Attr41</th>\n",
       "      <th>Attr42</th>\n",
       "      <th>Attr43</th>\n",
       "      <th>Attr44</th>\n",
       "      <th>Attr45</th>\n",
       "      <th>Attr46</th>\n",
       "      <th>Attr47</th>\n",
       "      <th>Attr48</th>\n",
       "      <th>Attr49</th>\n",
       "      <th>Attr50</th>\n",
       "      <th>Attr51</th>\n",
       "      <th>Attr52</th>\n",
       "      <th>Attr53</th>\n",
       "      <th>Attr54</th>\n",
       "      <th>Attr55</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>1.047800e+04</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>10500.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>1.049500e+04</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>9696.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10276.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>9.788000e+03</td>\n",
       "      <td>10275.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>1.040200e+04</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>5767.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>10301.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>9912.000000</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>1.041700e+04</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10489.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10417.000000</td>\n",
       "      <td>10275.000000</td>\n",
       "      <td>10275.000000</td>\n",
       "      <td>1.050300e+04</td>\n",
       "      <td>10460.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>10474.000000</td>\n",
       "      <td>10503.000000</td>\n",
       "      <td>9.911000e+03</td>\n",
       "      <td>10486.000000</td>\n",
       "      <td>1.046000e+04</td>\n",
       "      <td>10485.000000</td>\n",
       "      <td>10275.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.052844</td>\n",
       "      <td>0.619911</td>\n",
       "      <td>0.095490</td>\n",
       "      <td>9.980499</td>\n",
       "      <td>-1.347662e+03</td>\n",
       "      <td>-0.121159</td>\n",
       "      <td>0.065624</td>\n",
       "      <td>19.140113</td>\n",
       "      <td>1.819254</td>\n",
       "      <td>0.366093</td>\n",
       "      <td>0.086764</td>\n",
       "      <td>2.411266</td>\n",
       "      <td>0.376563</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>3.004332e+03</td>\n",
       "      <td>2.729654</td>\n",
       "      <td>20.511539</td>\n",
       "      <td>0.070739</td>\n",
       "      <td>-0.170778</td>\n",
       "      <td>68.448531</td>\n",
       "      <td>4.670725</td>\n",
       "      <td>0.075676</td>\n",
       "      <td>-0.176466</td>\n",
       "      <td>0.211936</td>\n",
       "      <td>0.196153</td>\n",
       "      <td>2.580730</td>\n",
       "      <td>1.185945e+03</td>\n",
       "      <td>6.092884</td>\n",
       "      <td>3.921179</td>\n",
       "      <td>0.459346</td>\n",
       "      <td>-0.177084</td>\n",
       "      <td>1.171670e+03</td>\n",
       "      <td>8.419887</td>\n",
       "      <td>5.398356</td>\n",
       "      <td>0.071075</td>\n",
       "      <td>1.981250</td>\n",
       "      <td>102.697692</td>\n",
       "      <td>0.465510</td>\n",
       "      <td>-0.076355</td>\n",
       "      <td>2.381026</td>\n",
       "      <td>28.707178</td>\n",
       "      <td>-0.141671</td>\n",
       "      <td>195.389286</td>\n",
       "      <td>126.940260</td>\n",
       "      <td>17.451297</td>\n",
       "      <td>8.978029</td>\n",
       "      <td>5.424888e+02</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>-0.217844</td>\n",
       "      <td>8.686024</td>\n",
       "      <td>0.497135</td>\n",
       "      <td>11.244158</td>\n",
       "      <td>5.725829</td>\n",
       "      <td>6.708568</td>\n",
       "      <td>6.638549e+03</td>\n",
       "      <td>-0.530082</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>3.848794</td>\n",
       "      <td>1.429319</td>\n",
       "      <td>5.713363e+02</td>\n",
       "      <td>13.935361</td>\n",
       "      <td>1.355370e+02</td>\n",
       "      <td>9.095149</td>\n",
       "      <td>35.766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.647797</td>\n",
       "      <td>6.427041</td>\n",
       "      <td>6.420056</td>\n",
       "      <td>523.691951</td>\n",
       "      <td>1.185806e+05</td>\n",
       "      <td>6.970625</td>\n",
       "      <td>0.651152</td>\n",
       "      <td>717.756745</td>\n",
       "      <td>7.581659</td>\n",
       "      <td>6.428603</td>\n",
       "      <td>0.655407</td>\n",
       "      <td>111.459642</td>\n",
       "      <td>49.675550</td>\n",
       "      <td>0.651151</td>\n",
       "      <td>1.091981e+05</td>\n",
       "      <td>110.233475</td>\n",
       "      <td>721.812766</td>\n",
       "      <td>0.835328</td>\n",
       "      <td>11.210201</td>\n",
       "      <td>1083.699904</td>\n",
       "      <td>305.897238</td>\n",
       "      <td>0.581734</td>\n",
       "      <td>11.149274</td>\n",
       "      <td>2.093845</td>\n",
       "      <td>6.742161</td>\n",
       "      <td>108.823993</td>\n",
       "      <td>3.469123e+04</td>\n",
       "      <td>94.713753</td>\n",
       "      <td>0.840528</td>\n",
       "      <td>71.755042</td>\n",
       "      <td>11.157003</td>\n",
       "      <td>6.841270e+04</td>\n",
       "      <td>37.696764</td>\n",
       "      <td>69.035263</td>\n",
       "      <td>0.572124</td>\n",
       "      <td>2.630705</td>\n",
       "      <td>2469.447679</td>\n",
       "      <td>6.425362</td>\n",
       "      <td>8.079334</td>\n",
       "      <td>37.819839</td>\n",
       "      <td>2845.290346</td>\n",
       "      <td>10.787555</td>\n",
       "      <td>3303.309554</td>\n",
       "      <td>2766.505503</td>\n",
       "      <td>1720.342176</td>\n",
       "      <td>523.565601</td>\n",
       "      <td>3.126416e+04</td>\n",
       "      <td>0.629770</td>\n",
       "      <td>11.910935</td>\n",
       "      <td>522.528325</td>\n",
       "      <td>6.420885</td>\n",
       "      <td>851.867915</td>\n",
       "      <td>89.708042</td>\n",
       "      <td>93.511530</td>\n",
       "      <td>5.989196e+04</td>\n",
       "      <td>55.978608</td>\n",
       "      <td>18.684047</td>\n",
       "      <td>190.201224</td>\n",
       "      <td>77.273270</td>\n",
       "      <td>3.715967e+04</td>\n",
       "      <td>83.704103</td>\n",
       "      <td>2.599116e+04</td>\n",
       "      <td>31.419096</td>\n",
       "      <td>428.298315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-17.692000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-479.730000</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>-1.190300e+07</td>\n",
       "      <td>-508.120000</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-2.081800</td>\n",
       "      <td>-1.215700</td>\n",
       "      <td>-479.730000</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-1543.800000</td>\n",
       "      <td>-631.710000</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-2.321800e+06</td>\n",
       "      <td>-204.300000</td>\n",
       "      <td>-0.043411</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-771.650000</td>\n",
       "      <td>-0.001439</td>\n",
       "      <td>-1.107500</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-771.650000</td>\n",
       "      <td>-60.742000</td>\n",
       "      <td>-500.750000</td>\n",
       "      <td>-204.300000</td>\n",
       "      <td>-1.901300e+05</td>\n",
       "      <td>-690.400000</td>\n",
       "      <td>-0.358530</td>\n",
       "      <td>-6351.700000</td>\n",
       "      <td>-771.390000</td>\n",
       "      <td>-9.295600e+03</td>\n",
       "      <td>-1.921900</td>\n",
       "      <td>-1696.000000</td>\n",
       "      <td>-17.073000</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-2.200900</td>\n",
       "      <td>-479.730000</td>\n",
       "      <td>-551.110000</td>\n",
       "      <td>-7.081900</td>\n",
       "      <td>-667.730000</td>\n",
       "      <td>-765.800000</td>\n",
       "      <td>-25113.000000</td>\n",
       "      <td>-25113.000000</td>\n",
       "      <td>-74385.000000</td>\n",
       "      <td>-6.469200</td>\n",
       "      <td>-1.730300e+01</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-905.750000</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25.467000</td>\n",
       "      <td>-869.040000</td>\n",
       "      <td>-706.490000</td>\n",
       "      <td>-7.513800e+05</td>\n",
       "      <td>-5691.700000</td>\n",
       "      <td>-1667.300000</td>\n",
       "      <td>-198.690000</td>\n",
       "      <td>-172.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-6.590300</td>\n",
       "      <td>-2.336500e+06</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.253955</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>1.040100</td>\n",
       "      <td>-5.207075e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.431270</td>\n",
       "      <td>1.011275</td>\n",
       "      <td>0.297340</td>\n",
       "      <td>0.009809</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>1.865950e+02</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>1.449200</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>14.253750</td>\n",
       "      <td>0.796673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.014938</td>\n",
       "      <td>0.135695</td>\n",
       "      <td>0.055067</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.027178</td>\n",
       "      <td>3.393200</td>\n",
       "      <td>0.073850</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>4.471900e+01</td>\n",
       "      <td>2.785900</td>\n",
       "      <td>0.367460</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>1.043500</td>\n",
       "      <td>1.038600</td>\n",
       "      <td>0.426810</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.053666</td>\n",
       "      <td>0.022026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.627250</td>\n",
       "      <td>34.214750</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>0.602830</td>\n",
       "      <td>1.494100e+01</td>\n",
       "      <td>-0.048988</td>\n",
       "      <td>-0.036186</td>\n",
       "      <td>0.749370</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.122470</td>\n",
       "      <td>0.667285</td>\n",
       "      <td>0.950435</td>\n",
       "      <td>1.462100e+01</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.875560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.533150e+00</td>\n",
       "      <td>4.486075</td>\n",
       "      <td>4.073700e+01</td>\n",
       "      <td>3.062800</td>\n",
       "      <td>2.023350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.043034</td>\n",
       "      <td>0.464140</td>\n",
       "      <td>0.198560</td>\n",
       "      <td>1.605600</td>\n",
       "      <td>1.579300e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050945</td>\n",
       "      <td>1.111000</td>\n",
       "      <td>1.199000</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.068104</td>\n",
       "      <td>0.155880</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.050953</td>\n",
       "      <td>8.056900e+02</td>\n",
       "      <td>0.235140</td>\n",
       "      <td>2.152500</td>\n",
       "      <td>0.050953</td>\n",
       "      <td>0.032117</td>\n",
       "      <td>34.432500</td>\n",
       "      <td>0.955035</td>\n",
       "      <td>0.054633</td>\n",
       "      <td>0.027129</td>\n",
       "      <td>0.158370</td>\n",
       "      <td>0.377690</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>1.066900e+00</td>\n",
       "      <td>0.459930</td>\n",
       "      <td>3.932100</td>\n",
       "      <td>0.216525</td>\n",
       "      <td>0.039689</td>\n",
       "      <td>7.752200e+01</td>\n",
       "      <td>4.680900</td>\n",
       "      <td>2.051600</td>\n",
       "      <td>0.053054</td>\n",
       "      <td>1.586900</td>\n",
       "      <td>2.727300</td>\n",
       "      <td>0.628750</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.079126</td>\n",
       "      <td>0.034909</td>\n",
       "      <td>99.131000</td>\n",
       "      <td>54.422000</td>\n",
       "      <td>0.261265</td>\n",
       "      <td>1.060600</td>\n",
       "      <td>3.710400e+01</td>\n",
       "      <td>0.009653</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>1.231200</td>\n",
       "      <td>0.326570</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>1.201800</td>\n",
       "      <td>1.373500</td>\n",
       "      <td>8.822900e+02</td>\n",
       "      <td>0.051765</td>\n",
       "      <td>0.106880</td>\n",
       "      <td>0.953060</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>9.952100e+00</td>\n",
       "      <td>6.677300</td>\n",
       "      <td>7.066400e+01</td>\n",
       "      <td>5.139200</td>\n",
       "      <td>4.059300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.123805</td>\n",
       "      <td>0.689330</td>\n",
       "      <td>0.419545</td>\n",
       "      <td>2.959500</td>\n",
       "      <td>5.608400e+01</td>\n",
       "      <td>0.072584</td>\n",
       "      <td>0.142275</td>\n",
       "      <td>2.857100</td>\n",
       "      <td>2.059100</td>\n",
       "      <td>0.725635</td>\n",
       "      <td>0.161090</td>\n",
       "      <td>0.570340</td>\n",
       "      <td>0.133612</td>\n",
       "      <td>0.142275</td>\n",
       "      <td>2.187100e+03</td>\n",
       "      <td>0.643030</td>\n",
       "      <td>3.924300</td>\n",
       "      <td>0.142285</td>\n",
       "      <td>0.087583</td>\n",
       "      <td>63.698250</td>\n",
       "      <td>1.105650</td>\n",
       "      <td>0.145890</td>\n",
       "      <td>0.075935</td>\n",
       "      <td>0.373673</td>\n",
       "      <td>0.614100</td>\n",
       "      <td>0.589080</td>\n",
       "      <td>4.814300e+00</td>\n",
       "      <td>1.564850</td>\n",
       "      <td>4.449950</td>\n",
       "      <td>0.426240</td>\n",
       "      <td>0.099040</td>\n",
       "      <td>1.291850e+02</td>\n",
       "      <td>8.081500</td>\n",
       "      <td>4.768600</td>\n",
       "      <td>0.145730</td>\n",
       "      <td>2.380750</td>\n",
       "      <td>10.741500</td>\n",
       "      <td>0.788415</td>\n",
       "      <td>0.090918</td>\n",
       "      <td>0.742150</td>\n",
       "      <td>0.195200</td>\n",
       "      <td>0.091339</td>\n",
       "      <td>141.482500</td>\n",
       "      <td>80.831500</td>\n",
       "      <td>0.949803</td>\n",
       "      <td>2.042600</td>\n",
       "      <td>6.982300e+01</td>\n",
       "      <td>0.100710</td>\n",
       "      <td>0.059706</td>\n",
       "      <td>2.306300</td>\n",
       "      <td>0.525190</td>\n",
       "      <td>0.353180</td>\n",
       "      <td>2.268900</td>\n",
       "      <td>2.437150</td>\n",
       "      <td>4.348900e+03</td>\n",
       "      <td>0.130010</td>\n",
       "      <td>0.271310</td>\n",
       "      <td>0.995927</td>\n",
       "      <td>0.240320</td>\n",
       "      <td>2.093600e+01</td>\n",
       "      <td>10.587500</td>\n",
       "      <td>1.182200e+02</td>\n",
       "      <td>8.882600</td>\n",
       "      <td>9.682750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>52.652000</td>\n",
       "      <td>480.730000</td>\n",
       "      <td>17.708000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>6.854400e+05</td>\n",
       "      <td>45.533000</td>\n",
       "      <td>52.652000</td>\n",
       "      <td>53432.000000</td>\n",
       "      <td>740.440000</td>\n",
       "      <td>11.837000</td>\n",
       "      <td>52.652000</td>\n",
       "      <td>8259.400000</td>\n",
       "      <td>4972.000000</td>\n",
       "      <td>52.652000</td>\n",
       "      <td>1.023600e+07</td>\n",
       "      <td>8259.400000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>53.689000</td>\n",
       "      <td>123.940000</td>\n",
       "      <td>91600.000000</td>\n",
       "      <td>29907.000000</td>\n",
       "      <td>47.597000</td>\n",
       "      <td>123.940000</td>\n",
       "      <td>179.920000</td>\n",
       "      <td>8.834500</td>\n",
       "      <td>8262.300000</td>\n",
       "      <td>2.723000e+06</td>\n",
       "      <td>6233.300000</td>\n",
       "      <td>9.619900</td>\n",
       "      <td>2940.500000</td>\n",
       "      <td>60.430000</td>\n",
       "      <td>6.674200e+06</td>\n",
       "      <td>2787.900000</td>\n",
       "      <td>6348.500000</td>\n",
       "      <td>47.597000</td>\n",
       "      <td>169.500000</td>\n",
       "      <td>136090.000000</td>\n",
       "      <td>13.656000</td>\n",
       "      <td>293.150000</td>\n",
       "      <td>2883.000000</td>\n",
       "      <td>288770.000000</td>\n",
       "      <td>165.950000</td>\n",
       "      <td>254030.000000</td>\n",
       "      <td>254030.000000</td>\n",
       "      <td>113280.000000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>2.591100e+06</td>\n",
       "      <td>47.597000</td>\n",
       "      <td>178.890000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>480.730000</td>\n",
       "      <td>84827.000000</td>\n",
       "      <td>6234.300000</td>\n",
       "      <td>6234.300000</td>\n",
       "      <td>3.380500e+06</td>\n",
       "      <td>293.150000</td>\n",
       "      <td>552.640000</td>\n",
       "      <td>18118.000000</td>\n",
       "      <td>7617.300000</td>\n",
       "      <td>3.660200e+06</td>\n",
       "      <td>4470.400000</td>\n",
       "      <td>1.073500e+06</td>\n",
       "      <td>1974.500000</td>\n",
       "      <td>21499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Attr1         Attr2         Attr3         Attr4         Attr5  \\\n",
       "count  10503.000000  10503.000000  10503.000000  10485.000000  1.047800e+04   \n",
       "mean       0.052844      0.619911      0.095490      9.980499 -1.347662e+03   \n",
       "std        0.647797      6.427041      6.420056    523.691951  1.185806e+05   \n",
       "min      -17.692000      0.000000   -479.730000      0.002080 -1.190300e+07   \n",
       "25%        0.000686      0.253955      0.017461      1.040100 -5.207075e+01   \n",
       "50%        0.043034      0.464140      0.198560      1.605600  1.579300e+00   \n",
       "75%        0.123805      0.689330      0.419545      2.959500  5.608400e+01   \n",
       "max       52.652000    480.730000     17.708000  53433.000000  6.854400e+05   \n",
       "\n",
       "              Attr6         Attr7         Attr8         Attr9        Attr10  \\\n",
       "count  10503.000000  10503.000000  10489.000000  10500.000000  10503.000000   \n",
       "mean      -0.121159      0.065624     19.140113      1.819254      0.366093   \n",
       "std        6.970625      0.651152    717.756745      7.581659      6.428603   \n",
       "min     -508.120000    -17.692000     -2.081800     -1.215700   -479.730000   \n",
       "25%        0.000000      0.002118      0.431270      1.011275      0.297340   \n",
       "50%        0.000000      0.050945      1.111000      1.199000      0.515500   \n",
       "75%        0.072584      0.142275      2.857100      2.059100      0.725635   \n",
       "max       45.533000     52.652000  53432.000000    740.440000     11.837000   \n",
       "\n",
       "             Attr11        Attr12        Attr13        Attr14        Attr15  \\\n",
       "count  10503.000000  10485.000000  10460.000000  10503.000000  1.049500e+04   \n",
       "mean       0.086764      2.411266      0.376563      0.065634  3.004332e+03   \n",
       "std        0.655407    111.459642     49.675550      0.651151  1.091981e+05   \n",
       "min      -17.692000  -1543.800000   -631.710000    -17.692000 -2.321800e+06   \n",
       "25%        0.009809      0.006183      0.020740      0.002131  1.865950e+02   \n",
       "50%        0.068104      0.155880      0.066433      0.050953  8.056900e+02   \n",
       "75%        0.161090      0.570340      0.133612      0.142275  2.187100e+03   \n",
       "max       52.652000   8259.400000   4972.000000     52.652000  1.023600e+07   \n",
       "\n",
       "             Attr16        Attr17        Attr18        Attr19        Attr20  \\\n",
       "count  10489.000000  10489.000000  10503.000000  10460.000000  10460.000000   \n",
       "mean       2.729654     20.511539      0.070739     -0.170778     68.448531   \n",
       "std      110.233475    721.812766      0.835328     11.210201   1083.699904   \n",
       "min     -204.300000     -0.043411    -17.692000   -771.650000     -0.001439   \n",
       "25%        0.060302      1.449200      0.002131      0.001648     14.253750   \n",
       "50%        0.235140      2.152500      0.050953      0.032117     34.432500   \n",
       "75%        0.643030      3.924300      0.142285      0.087583     63.698250   \n",
       "max     8259.400000  53433.000000     53.689000    123.940000  91600.000000   \n",
       "\n",
       "             Attr21        Attr22        Attr23        Attr24        Attr25  \\\n",
       "count   9696.000000  10503.000000  10460.000000  10276.000000  10503.000000   \n",
       "mean       4.670725      0.075676     -0.176466      0.211936      0.196153   \n",
       "std      305.897238      0.581734     11.149274      2.093845      6.742161   \n",
       "min       -1.107500    -17.692000   -771.650000    -60.742000   -500.750000   \n",
       "25%        0.796673      0.000000      0.000556      0.014938      0.135695   \n",
       "50%        0.955035      0.054633      0.027129      0.158370      0.377690   \n",
       "75%        1.105650      0.145890      0.075935      0.373673      0.614100   \n",
       "max    29907.000000     47.597000    123.940000    179.920000      8.834500   \n",
       "\n",
       "             Attr26        Attr27        Attr28        Attr29        Attr30  \\\n",
       "count  10489.000000  9.788000e+03  10275.000000  10503.000000  10460.000000   \n",
       "mean       2.580730  1.185945e+03      6.092884      3.921179      0.459346   \n",
       "std      108.823993  3.469123e+04     94.713753      0.840528     71.755042   \n",
       "min     -204.300000 -1.901300e+05   -690.400000     -0.358530  -6351.700000   \n",
       "25%        0.055067  0.000000e+00      0.027178      3.393200      0.073850   \n",
       "50%        0.213260  1.066900e+00      0.459930      3.932100      0.216525   \n",
       "75%        0.589080  4.814300e+00      1.564850      4.449950      0.426240   \n",
       "max     8262.300000  2.723000e+06   6233.300000      9.619900   2940.500000   \n",
       "\n",
       "             Attr31        Attr32        Attr33        Attr34        Attr35  \\\n",
       "count  10460.000000  1.040200e+04  10485.000000  10489.000000  10503.000000   \n",
       "mean      -0.177084  1.171670e+03      8.419887      5.398356      0.071075   \n",
       "std       11.157003  6.841270e+04     37.696764     69.035263      0.572124   \n",
       "min     -771.390000 -9.295600e+03     -1.921900  -1696.000000    -17.073000   \n",
       "25%        0.004012  4.471900e+01      2.785900      0.367460      0.001730   \n",
       "50%        0.039689  7.752200e+01      4.680900      2.051600      0.053054   \n",
       "75%        0.099040  1.291850e+02      8.081500      4.768600      0.145730   \n",
       "max       60.430000  6.674200e+06   2787.900000   6348.500000     47.597000   \n",
       "\n",
       "             Attr36         Attr37        Attr38        Attr39        Attr40  \\\n",
       "count  10503.000000    5767.000000  10503.000000  10460.000000  10485.000000   \n",
       "mean       1.981250     102.697692      0.465510     -0.076355      2.381026   \n",
       "std        2.630705    2469.447679      6.425362      8.079334     37.819839   \n",
       "min       -0.000084      -2.200900   -479.730000   -551.110000     -7.081900   \n",
       "25%        1.043500       1.038600      0.426810      0.001557      0.053666   \n",
       "50%        1.586900       2.727300      0.628750      0.034080      0.195900   \n",
       "75%        2.380750      10.741500      0.788415      0.090918      0.742150   \n",
       "max      169.500000  136090.000000     13.656000    293.150000   2883.000000   \n",
       "\n",
       "              Attr41        Attr42         Attr43         Attr44  \\\n",
       "count   10301.000000  10460.000000   10460.000000   10460.000000   \n",
       "mean       28.707178     -0.141671     195.389286     126.940260   \n",
       "std      2845.290346     10.787555    3303.309554    2766.505503   \n",
       "min      -667.730000   -765.800000  -25113.000000  -25113.000000   \n",
       "25%         0.022026      0.000000      65.627250      34.214750   \n",
       "50%         0.079126      0.034909      99.131000      54.422000   \n",
       "75%         0.195200      0.091339     141.482500      80.831500   \n",
       "max    288770.000000    165.950000  254030.000000  254030.000000   \n",
       "\n",
       "              Attr45        Attr46        Attr47        Attr48        Attr49  \\\n",
       "count    9912.000000  10485.000000  1.041700e+04  10503.000000  10460.000000   \n",
       "mean       17.451297      8.978029  5.424888e+02      0.004834     -0.217844   \n",
       "std      1720.342176    523.565601  3.126416e+04      0.629770     11.910935   \n",
       "min    -74385.000000     -6.469200 -1.730300e+01    -17.692000   -905.750000   \n",
       "25%         0.006157      0.602830  1.494100e+01     -0.048988     -0.036186   \n",
       "50%         0.261265      1.060600  3.710400e+01      0.009653      0.006034   \n",
       "75%         0.949803      2.042600  6.982300e+01      0.100710      0.059706   \n",
       "max    113280.000000  53433.000000  2.591100e+06     47.597000    178.890000   \n",
       "\n",
       "             Attr50        Attr51        Attr52        Attr53        Attr54  \\\n",
       "count  10489.000000  10503.000000  10417.000000  10275.000000  10275.000000   \n",
       "mean       8.686024      0.497135     11.244158      5.725829      6.708568   \n",
       "std      522.528325      6.420885    851.867915     89.708042     93.511530   \n",
       "min        0.002080      0.000000    -25.467000   -869.040000   -706.490000   \n",
       "25%        0.749370      0.177200      0.122470      0.667285      0.950435   \n",
       "50%        1.231200      0.326570      0.212000      1.201800      1.373500   \n",
       "75%        2.306300      0.525190      0.353180      2.268900      2.437150   \n",
       "max    53433.000000    480.730000  84827.000000   6234.300000   6234.300000   \n",
       "\n",
       "             Attr55        Attr56        Attr57        Attr58        Attr59  \\\n",
       "count  1.050300e+04  10460.000000  10503.000000  10474.000000  10503.000000   \n",
       "mean   6.638549e+03     -0.530082     -0.014817      3.848794      1.429319   \n",
       "std    5.989196e+04     55.978608     18.684047    190.201224     77.273270   \n",
       "min   -7.513800e+05  -5691.700000  -1667.300000   -198.690000   -172.070000   \n",
       "25%    1.462100e+01      0.005137      0.006796      0.875560      0.000000   \n",
       "50%    8.822900e+02      0.051765      0.106880      0.953060      0.002976   \n",
       "75%    4.348900e+03      0.130010      0.271310      0.995927      0.240320   \n",
       "max    3.380500e+06    293.150000    552.640000  18118.000000   7617.300000   \n",
       "\n",
       "             Attr60        Attr61        Attr62        Attr63        Attr64  \n",
       "count  9.911000e+03  10486.000000  1.046000e+04  10485.000000  10275.000000  \n",
       "mean   5.713363e+02     13.935361  1.355370e+02      9.095149     35.766800  \n",
       "std    3.715967e+04     83.704103  2.599116e+04     31.419096    428.298315  \n",
       "min    0.000000e+00     -6.590300 -2.336500e+06     -0.000156     -0.000102  \n",
       "25%    5.533150e+00      4.486075  4.073700e+01      3.062800      2.023350  \n",
       "50%    9.952100e+00      6.677300  7.066400e+01      5.139200      4.059300  \n",
       "75%    2.093600e+01     10.587500  1.182200e+02      8.882600      9.682750  \n",
       "max    3.660200e+06   4470.400000  1.073500e+06   1974.500000  21499.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b0928",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{clearly this dataset contains outliers as seen my max values and 75% percentile values, and} \\\\ \\textit{missing values indicated by count, we first explore the missing values and then move to outlier.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1614011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0'    10008\n",
      "b'1'      495\n",
      "Name: class, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEoCAYAAABy03fpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAie0lEQVR4nO3de7xUVf3/8ddb8G6mBt5ARZNfClqmfPlaWpp+v0laeUkNy8Qk8Ytamllevmp0Ia349k0rLTUFzPJLakkXK6PMUpLwkih4QUVBUfBWeMPAz++PtUY2c+acM4d9zgzDeT8fj3mc2WuvvWftmX3mM2utvddSRGBmZraq1mp2AczMrLU5kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4k1jIkzZN0erPL0RlJgySFpGE9sO9xku4tLE+U9Mvufp287x47DluzOJDYakHSFpIulPSwpKWSnpB0o6QDm122ivylWnm8LOkRST+WtHdV1vnAVsDdde63KwFyArBP/aWuj6SbJX23KrlLx2G9lwOJNZ2kQcCdwAHAWcDbgf8AfgV8v3klq+l40pfrzsBo4DXgFkmfr2SIiOUR8VRELOuuF5W0lqQ+EfFiRDzbXfvtSE8ch62ZHEhsdXAxIGBYREyJiAciYk5EfBd4R3sbSTpN0j2SXso1mMslbVJY/2ZJV0laJOnVXIM4tbD+BEkP5nWLJf1WUt9OyvpC/nJ9LCL+GBHHAhcA50vaMe93pSYhSWtLukjSk7m2NV/SBXndzcB2wDcrtZ2cfqykFyUdmJuyXgN2rm7aKhzLOZKezttcKWn9wro2tY1ik5ikiaRazkmFGtegWk1bkt4r6fb8nj0t6X8lrVP1WhdL+pqkZ/J7P0HSWoU8h+XP7RVJz0n6k6QtOnnfbTXmQGJNJWkzYATw3Yh4sXp9RDzfweavA6cCQ4GPAcOB7xTWfxXYFfggsBNwHPBEft1hwPeALwFvI9WAfrOKh/E/pP+lQ9pZ/xngUGAkMBj4KPBAXncYsAD4Mqmms1Vhu/WAc4ATgCHAY+3sfx9SwN0f+AjwfuDrXSj/KcB04MpCGeZXZ5I0ALgRuAt4J6lGdhRwflXWjwPLgHcDJ5M+o4/mfWwJXANMItXq3gtc1YWy2mqos19fZj1tR1JtZE5XN4yIbxcW50n6AnCDpFER8Trpl/5dETGjkqeQf1vgJWBqRCwhfUn/vevFh4h4VtIiYId2smwHPAj8OdLgdo8Dt+Vtn5O0HFgSEU9VbdcH+HRE3FFJkFRr/8uBT+ZAfK+kM4AfSjorIl6qo/z/kPQa8HKxDDVe60RgIXBifn/nSDoT+IGkcyPi5ZxvdkScl58/KOl4UpD7CbA1sDZwbURUAmObGpa1FtdIrNlqfjPWtaG0n6SbJC2QtAS4HlgH2DJnuQQ4UtLfc/NKsZP6JlLweFTS1ZJGSXrTqpaFdBztjYA6EdiN9KX6PUkHFZt6OrCM+jq676mqzU0nvQ9vrWPbrtgZmJ6DSMVf8mvtWCxP1XZPApvn538Hfk8KeNdJGiupfzeX0xrMgcSa7SHSF/DOXdlI0nakzvg5wBHAHqSmK0hfbETEjaTawASgH/ArSVfmdUuA3YEjSTWEs4D7JW3d1QOQ1A/oDzxSa31E3AkMAs4m/c9NAm6qI5gsjYjlXS1PDa/TNmCvvQr76ShYFtP/VWPdWpA68ElNb+8nBZzRwEOS2u0Ls9WfA4k1VUQ8B/wWOFnSRtXri53nVYaRAsZnI2J6RDxIajap3v8zEXFV7hQfDYyStG5etywi/hARlSvFNiT1p3TV50hf1je0lyEilkTETyNiLHAQsB8rfsW/RmrGWlW7StqwsLxn3ufDeXkxK/e9QNuLGOopw2zgXVUBcO+q1+pUJNMj4kvAv5FqLB+td3tb/biPxFYHJ5L6DGZKOpf0S1XA+0g1hW1rbPMQ6YfQqZKuJ315nlrMIOnLpMuK7yOd64cBj0TEUkkfJDX93AI8l1/rTXTeV7NJ7jCuNB2NAo4BvhARc2ttIOk0Ut/C3aRf6x8D/knqZIfUd/MeST8i1UKe6aQM1foCV+Tj3Zp0Fdllhf6RPwDflvRhUif/CcA2rNxnNA8YrnQp9ouk96TaxaT3+GJJF5L6hC4gXSjxco38bUjak3Rhw2+Bp0md9tuQgpS1KAcSa7qIeFTS7qSmn68DA4BnSe3pJ7SzzT2STgHOIF2ddRtwOvB/hWxLgfHA9sCrwF+BD+V1L5CusjoP2ID0i/pTEfHnTop7WWHfC/M+942IWzrYZgnwedIVW0G66ukDhS/f84Af5DKsS9f7jf5ECpZ/zMdyHfCFwvorSDWuK/LyxcDPSM19FRNITW6zgfVJ79lKIuIJSR8AvkkKii8APyZ9bvX6B7AX8GlgE9LVYV+JiB91YR+2mpFnSDQzszLcR2JmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQtJg8+usVnefsvfKItYc3uxw9TWkek30Ly/tKmte0AvWQykjIzS5Hd5K0q9KI1Rt2nnv150DSQiRtDpxGum+imH6ipEfz0N53SHrPKux7XP4CvrwqvVtmydPKk0L9K38JXqDOh21vKfnLPPKwKasdSUMlXas0pH5IGreK+zm26jNdImmGpIO6uchNpx6YhTIiZpHuQTqtO/fbLA4kreVTwIyIeGNMJ0kfBS4Evka6S/g24EZJte4G78yrwLGShnZHYWuoTAq1A+kO6bGk4UWaqjifRi+wAeku9nOAR0vu62VWDDu/B+ncu34Vz71uJWlVxhJrtCuBsWvCjykHktbyMWBqVdppwMSIuCxPBvVp0h3XY1dh/w+Thq6onl9iJbla/vvCxEQTJb25jv1XJoWaHxE/J43Au3thv2+VdIOkp5Qmq7ozD2VSfO15SpM4/UDSP/PIv5+nA5LOUJpk6d8L+xgn6QpJLwBXt1fzKjaTFfJ8TNJfcg3wfknvr6wn3V0OsDjnnZjXSdLnJD2kNLnVAknn53V/UNuJpzZWms73sDre17pFxN8i4vSI+DEpEJTcXTyVHw+SgtM6pPlhAJB0tKS/5RrLIkk/VZrXpLK+UoPbX2nCrJclzcwjHdQkaVNJtypNRLZhYR8H5lrRa8ABqjEJWHUzWSWPpE9Jejyf0z+v1ChzjW0UcFCh9rVvXre10sjRz+Zy3y3pffk8WV7jXDo+n4eVHy6/AzYD9u3yO7+acSBpEUoTQA0BZhbS1iH9EvxdVfbfkSYVquQbpzzzXh3OJP3T1Gwek7QBaQKoF0kTSR2aX6tL/TaShuTtbi8kb0SaOOk/SYMKXkf6hbtT1eafBWaRgtDXgW9IeleN15CkCaThOPaJiOJrnQbcTxr8sStDfAB8A7iINDT8TaQ5UAaQhvv4SM4zlPRL/ZS8/DXgXFKQHkoasbgyedRlwMeUB5PMjiK9x7/oYtlK04pmq0Fd2KYv8ElSrbY4r8s6wBdJn+cHScOy/KTGLs4nnXu7k4bHuVpqOyGKpK1I46M9AXyoar6Vr5OC2U6sfF51ZhBwNHAwaRywwaw4nycAU0hD31dqX7cp9W38KW97KGkCtS8DRMS8nL8yGnXFccBVEfFazvcaaaiZfWh1EeFHCzxIX1oBbF9I2zqnvbcq73nAA4Xlk4H7O9n/OODe/PxK0rwTkP5RgjQNLqTmqX8Abypsu2/Os2MH+w/gFdKX46t5+adAn07K9VfgnMLyPOAnVXkeqsoTpNFkryRNKDWoKv884BdVaSsdZ9W+Dq/K89+F9Wvl1/hq1XvRr5Bno3zM/9XOMa4LPAOMLKTdDkzo5L2ZRxrnq/g5zOvCOXUvMK5G+qGkIDugg22Pzcf5Yn4sJ9VwRnbymjvl7QZWvV8HFPLsVZXn2PwaO5KG6r8EWKvG+feR9s7pqnK/WJVnObBtIW3vvL/BeXki8Muq/RxPGkOtXzvHeTjwPLBeXt4573OXqnzXk4JL079jyjxcI2kdlTm4X62xrrq2sdK8ERHx3Yio/lXfkfOA3dppVtmZNJHSkkLabaRh1Id0st/PkwJi5dfp20kDBaZCp2aKb0iaLen53AQxjLaj/3Y0cVLFBNIXzN6RfiFWm1kjrV7TK08iTfJ0Ox0f+xBSsJhWa2VELCVNN3scvFFbG04Xa3ndJSJ+FhE7RcQTnWR9mfR57kbqnzsHuFLSgZUMknZXaq58TGnyscr73tFn+mT+W/xM1yFNonVjRIyNlSfXqljVz/SJiHi8sHw76XzuaI6cd5L+D9obqfkG0vD6lf+h40j9m9WzQb7Civ/tluVA0joqJ+ymVWnLWTEjYMXmpCG6V0lEzCfNfX4+bUeIrndyo1qeioi5EfFARPyK1OTxcUmVmfwmkJp8ziVV93cDZpAnqipod+KkgptI78uB1FY9BW3li+mN5hR1X4dtPaP5Xg7sr9RRPZpUI1zdh1aP/HnOjYh7IuJbpOaesyD9MCD1ub0MfII098iIvG1Hn2nlPFqrav3vgAOVJjWrpdZn2h0TetXS4WcaEf8CJgPH5Wa/TwA/rJF1M9J8MS3NgaR1PEyaw+KNX76R2ljvIPUpFP0neU7wEs4nzfr3qar02cA7tPK0tO8mnUtdnXe9MvvfBvnv3sDkiLguIu4hzdexqtPF/poUlC6RNKqO/JV/5uIEULu1k3fPypPcjj+cFcf+Wv5bnCRqNmnY+f3be/GIuI/0S/h4Unt9q94rtJwVn+dOpD6RsyPiloi4n7Y1x3oFqVnqL8AfVd+VYYuBLar6WnarkW+ApG0Ky8NZ+XyuNenXncDb1fFl3peR5rk5kTTXzTU18uyS99XSHEhaRK7K/570ZVv0LdIlu5+StLPShENbA9+vZJB0sqT7u/h6z5M6iE+pWnU16ZffZKWrt95Lmkvj+mhnYqeCTSRtma922YfUhPYgK/5hHwQOzc0huwI/AtbrSrmrjuGXpGDyfUnHdJL3FVJ/zBlK91q8m1RDqmWspMMlvQ34Nmk630vyusdIX3oHSeovaaPcDHghcL6kTypdnTZcUvWVdZeR5hHZkJXnVek2ktaRtJuk3Ujv7ZZ5ecdCnkOVrkYb0O6O3siqLfNje0ljgANYMVPk46QAerKkHZTuMfnKqpY9/w+MIv1IurmOYHIz6Rf/2fk9H03qu6j2CjApvw/vIv3v/CoiHsrr5wG7SHqbpH65pvpjYBHwc0nvycf/YUnvK5T3QVLg+yZwbUT8s/ii+WKGAbS9WKb1NLuTxo/6H6R/0oVUdVCTfvHMI/3T3kHbzvdx6aPucN/jaNsxuS4rvhiHFdJ3JbX3v0LqUJwIvLmT/Ufh8TqpHfwaYIdCnu1IwfIlUm3kdOCXpMubK3nmAadX7ftm0ix9xdc6vLD8oVzWY9rbR07fGbiV1BQzC3hPcV+s6Gz/OOnL7FXSjIMfqNrPuflzer1SdtKPtjNJncWvka7YGl+13QakDtwr6jwf5tHFzvbCMVQ/bi7kOTanDepgP8dWbV95L84unp+kix4ezutnkM7hqJSb2hcnVMo4rPBaxQ7yPqQfGQ+T+lra7KOQ9wTSOfwS6Xw7hbad7fcCY/Jn8gopEPYv5OlP+rJfUlX2gaSA/0I+Z+4qfh45zzHUuCAmrzsL+E0jv0N66uGJrVqMpOnAxRFxVbPL0tvkX5CPAv8WEWU669vb/9akX/H7RMStdeSfBxwbETfn5X1JgWtQd5dtTaV0n8jhEbFLD+3/DGB0RPy/qvR1SVcbHlXPZ726c9NW6zkBf25rFElr52aarwN3rQlfLL2dpI3yDYmnkJo1q21HqpGuEZ91y9+a39tE6oSuvvzVWttepDviHwKObHJZrHt8l3RT6VRSH+JKIvWfPNjoQvUUN22ZtShJpwI/j3yfTG56OyQivt28Ullv5EBiZmal9LqmrX79+sWgQYOaXQwzs5Zyxx13PBMR/Wut63WBZNCgQcyc2e0X3JiZrdEkPdbeOl/9Y2ZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmal9FggUZoPe5EKcyZL2kzSTUrzVt8kadPCurMkzZX0gKQDCul7SJqV111UGRJa0rqS/i+n364uTAtqZmbdpydrJBNZMYlNxZnAtIgYTBo99kx4Y0a4kaS5rEcAF0uqjP9/CWlkzsH5UdnnaOD5iNgR+F/SOEVmZtZgPRZIIuIW4Lmq5INZMbXqJOCQQvo1EbE0Ih4F5gLDJW0FbBwR0yPdgj+5apvKvq4lzS5Xz0x0ZmbWjRrdR7JFRCwEyH8rs6UNIM0FULEgpw3Iz6vTV9omIpYB/wDe0mMlNzOzmlaXO9tr1SSig/SOtmm78zRz2xiAbbetZ4bOju3x+cml92Frnju+2eEkjGZrrEbXSJ7OzVXkv4ty+gKgOGfyQNIMegvy8+r0lbaR1Bd4M22b0gCIiEsjYlhEDOvfv+ZQMWZmtooaHUimkuZcJv+9oZA+Ml+JtT2pU31Gbv5aImnP3P9xTNU2lX0dDvwhPJSxmVnD9VjTlqSfkOZS7idpAfBF4AJgiqTRpClFjwCIiPskTQFmA8uAkyJied7VWNIVYOsDN+YHwA+BqyTNJdVERvbUsZiZWft6LJBExFHtrNq/nfzjgfE10mcCbeZTjohXyYHIzMyax3e2m5lZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU0JZBI+qyk+yTdK+knktaTtJmkmyQ9lP9uWsh/lqS5kh6QdEAhfQ9Js/K6iySpGcdjZtabNTyQSBoAfAYYFhG7AH2AkcCZwLSIGAxMy8tIGpLXDwVGABdL6pN3dwkwBhicHyMaeChmZkbzmrb6AutL6gtsADwJHAxMyusnAYfk5wcD10TE0oh4FJgLDJe0FbBxREyPiAAmF7YxM7MGaXggiYgngAnA48BC4B8R8Ttgi4hYmPMsBDbPmwwA5hd2sSCnDcjPq9PNzKyBmtG0tSmplrE9sDWwoaSjO9qkRlp0kF7rNcdImilp5uLFi7taZDMz60Azmrb+A3g0IhZHxL+A64F3A0/n5iry30U5/wJgm8L2A0lNYQvy8+r0NiLi0ogYFhHD+vfv360HY2bW2zUjkDwO7Clpg3yV1f7AHGAqMCrnGQXckJ9PBUZKWlfS9qRO9Rm5+WuJpD3zfo4pbGNmZg3St9EvGBG3S7oWuBNYBtwFXApsBEyRNJoUbI7I+e+TNAWYnfOfFBHL8+7GAhOB9YEb88PMzBqo4YEEICK+CHyxKnkpqXZSK/94YHyN9JnALt1eQDMzq5vvbDczs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvFgcTMzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvFgcTMzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSmhJIJG0i6VpJ90uaI+ldkjaTdJOkh/LfTQv5z5I0V9IDkg4opO8haVZed5EkNeN4zMx6s2bVSC4EfhMROwHvAOYAZwLTImIwMC0vI2kIMBIYCowALpbUJ+/nEmAMMDg/RjTyIMzMrAmBRNLGwHuBHwJExGsR8QJwMDApZ5sEHJKfHwxcExFLI+JRYC4wXNJWwMYRMT0iAphc2MbMzBqkGTWSHYDFwJWS7pJ0uaQNgS0iYiFA/rt5zj8AmF/YfkFOG5CfV6ebmVkDNSOQ9AV2By6JiHcCL5GbsdpRq98jOkhvuwNpjKSZkmYuXry4q+U1M7MONCOQLAAWRMTteflaUmB5OjdXkf8uKuTfprD9QODJnD6wRnobEXFpRAyLiGH9+/fvtgMxM7MmBJKIeAqYL+ltOWl/YDYwFRiV00YBN+TnU4GRktaVtD2pU31Gbv5aImnPfLXWMYVtzMysQfrWk0nStIjYv7O0Lvg0cLWkdYBHgE+SgtoUSaOBx4EjACLiPklTSMFmGXBSRCzP+xkLTATWB27MDzMza6AOA4mk9YANgH75vo5Kv8TGwNar+qIRcTcwrMaqmoEpIsYD42ukzwR2WdVymJlZeZ3VSE4ATiUFjTtYEUj+CXyv54plZmatosNAEhEXAhdK+nREfKdBZTIzsxZSVx9JRHxH0ruBQcVtImJyD5XLzMxaRL2d7VcBbwXuBiod3ZW7yc3MrBerK5CQOsaH5KFIzMzM3lDvfST3Alv2ZEHMzKw11Vsj6QfMljQDWFpJjIgP90ipzMysZdQbSMb1ZCHMzKx11XvV1p96uiBmZtaa6r1qawkrRtZdB1gbeCkiNu6pgpmZWWuot0bypuKypEOA4T1RIDMzay2rNPpvRPwc2K97i2JmZq2o3qatwwqLa5HuK/E9JWZmVvdVWx8qPF8GzCPNpW5mZr1cvX0kn+zpgpiZWWuqq49E0kBJP5O0SNLTkq6TNLDzLc3MbE1Xb2f7laQpb7cGBgC/yGlmZtbL1RtI+kfElRGxLD8mAv17sFxmZtYi6g0kz0g6WlKf/DgaeLYnC2ZmZq2h3kByHHAk8BSwEDgccAe8mZnVffnvV4BREfE8gKTNgAmkAGNmZr1YvTWSt1eCCEBEPAe8s2eKZGZmraTeQLKWpE0rC7lGUm9txszM1mD1BoP/AW6TdC1paJQjgfE9ViozM2sZ9d7ZPlnSTNJAjQIOi4jZPVoyMzNrCXU3T+XA4eBhZmYrWaVh5M3MzCocSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMyslKYFkjyK8F2SfpmXN5N0k6SH8t/infRnSZor6QFJBxTS95A0K6+7SJKacSxmZr1ZM2skpwBzCstnAtMiYjAwLS8jaQgwEhgKjAAultQnb3MJMAYYnB8jGlN0MzOraEogydP0HgRcXkg+GJiUn08CDimkXxMRSyPiUWAuMFzSVsDGETE9IgKYXNjGzMwapFk1km8DXwBeL6RtERELAfLfzXP6AGB+Id+CnDYgP69ONzOzBmp4IJH0QWBRRNxR7yY10qKD9FqvOUbSTEkzFy9eXOfLmplZPZpRI9kL+LCkecA1wH6SfgQ8nZuryH8X5fwLgG0K2w8EnszpA2uktxERl0bEsIgY1r+/p5o3M+tODQ8kEXFWRAyMiEGkTvQ/RMTRwFRgVM42CrghP58KjJS0rqTtSZ3qM3Lz1xJJe+artY4pbGNmZg2yOk1OdQEwRdJo4HHgCICIuE/SFNLIw8uAkyJied5mLDARWB+4MT/MzKyBmhpIIuJm4Ob8/Flg/3byjafGRFoRMRPYpedKaGZmnfGd7WZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDiRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiZmZleJAYmZmpTiQmJlZKQ4kZmZWigOJmZmV4kBiZmalOJCYmVkpDQ8kkraR9EdJcyTdJ+mUnL6ZpJskPZT/blrY5ixJcyU9IOmAQvoekmbldRdJUqOPx8yst2tGjWQZ8LmI2BnYEzhJ0hDgTGBaRAwGpuVl8rqRwFBgBHCxpD55X5cAY4DB+TGikQdiZmZNCCQRsTAi7szPlwBzgAHAwcCknG0ScEh+fjBwTUQsjYhHgbnAcElbARtHxPSICGByYRszM2uQpvaRSBoEvBO4HdgiIhZCCjbA5jnbAGB+YbMFOW1Afl6dbmZmDdS0QCJpI+A64NSI+GdHWWukRQfptV5rjKSZkmYuXry464U1M7N2NSWQSFqbFESujojrc/LTubmK/HdRTl8AbFPYfCDwZE4fWCO9jYi4NCKGRcSw/v37d9+BmJlZU67aEvBDYE5EfKuwaiowKj8fBdxQSB8paV1J25M61Wfk5q8lkvbM+zymsI2ZmTVI3ya85l7AJ4BZku7OaWcDFwBTJI0GHgeOAIiI+yRNAWaTrvg6KSKW5+3GAhOB9YEb88PMzBqo4YEkIv5C7f4NgP3b2WY8ML5G+kxgl+4rnZmZdZXvbDczs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvFgcTMzEpxIDEzs1IcSMzMrBQHEjMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKwUBxIzMyvFgcTMzEpxIDEzs1IcSMzMrBQHEjMzK6VvswtgZt3n8S/v2uwi2Gpo2/Nm9ej+XSMxM7NSHEjMzKwUBxIzMyvFgcTMzEpxIDEzs1JaPpBIGiHpAUlzJZ3Z7PKYmfU2LR1IJPUBvgd8ABgCHCVpSHNLZWbWu7R0IAGGA3Mj4pGIeA24Bji4yWUyM+tVWj2QDADmF5YX5DQzM2uQVr+zXTXSok0maQwwJi++KOmBHi1V79IPeKbZhVgdaMKoZhfBVuZzs+KLtb4qu2y79la0eiBZAGxTWB4IPFmdKSIuBS5tVKF6E0kzI2JYs8thVs3nZuO0etPW34DBkraXtA4wEpja5DKZmfUqLV0jiYhlkk4Gfgv0Aa6IiPuaXCwzs16lpQMJQET8Gvh1s8vRi7nJ0FZXPjcbRBFt+qbNzMzq1up9JGZm1mQOJNYuSYMk3Vsj/WZJg/LzPSTNykPUXCRJOX2cpGMbW2LrTeo8P8dLmi/pxao8Pj+7kQOJlXUJ6R6dwfkxornFMVvJL0gjYFgPciCxzvSVNEnSPZKulbQB8BywXNJWwMYRMT1SZ9tk4JC83YvAK80psvUi7Z6fABHx14hYWGM7n5/dqOWv2rIe9zZgdETcKukK4MSIOAxA0jDSTaEVbwxRExETGl5S643aPT874vOze7lGYp2ZHxG35uc/AvYurKtriBqzHtTR+WkN4kBinakODMXlBaRhaSpqDlFj1oM6Oj+tQRxIrDPbSnpXfn4U8JfKitz2vETSnvlqrWOAG5pQRuu92j0/rXEcSKwzc4BRku4BNiNdpVU0FrgcmAs8DNzY2OJZL9fh+SnpG5IWABtIWiBpXBPKuMbzne1mZlaKayRmZlaKA4mZmZXiQGJmZqU4kJiZWSkOJGZmVooDiVkD5VFnT292Ocy6kwOJmZmV4kBi1oMkHZNHpv27pKuq1h0v6W953XV55FokHSHp3px+S04bKmmGpLvz/gY343jMavENiWY9RNJQ4Hpgr4h4RtJmwGeAFyNigqS3RMSzOe9Xgacj4juSZgEjIuIJSZtExAuSvgP8NSKulrQO0CciPAy6rRZcIzHrOfsB10bEMwAR8VzV+l0k/TkHjo8DQ3P6rcBESccDfXLadOBsSWcA2zmI2OrEgcSs54iOR6OdCJwcEbsCXwLWA4iI/wLOAbYB7s41lx8DHyZNxvRbSfv1ZMHNusKBxKznTAOOlPQWgNy0VfQmYKGktUk1EnK+t0bE7RFxHvAMsI2kHYBHIuIiYCrw9oYcgVkdPEOiWQ+JiPskjQf+JGk5cBcwr5DlXOB24DFgFimwAHwzd6aLFIz+DpwJHC3pX8BTwJcbchBmdXBnu5mZleKmLTMzK8WBxMzMSnEgMTOzUhxIzMysFAcSMzMrxYHEzMxKcSAxM7NSHEjMzKyU/w/AERtl+4JpuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data=data,x='class')\n",
    "plt.title('Class Distributions \\n (0: No Bankruptcy || 1: Bankruptcy)', fontsize=14);\n",
    "print(data['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a66f0",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{Highly Imbalanced Data with around 20 times more data of non-bankrupt class}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb49a4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABA4AAAF6CAYAAACZVRowAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAk8klEQVR4nO3dfbxsd10f+s83DwgxiIQccyNBIwpUajXIKY/3XmMTNDxTngRLDBQaCiqkgm1AkCoUYguVVi+UiEgKWAjElhAeNASSWy5EOAmBAAEiMSW58pDEohWQK/C7f6x1wj77nN+e2Xv2njlz5v1+vea118ya73y+Z+/fXif5npm1qrUWAAAAgAM5bNENAAAAAAcvgwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAICuI+YZduyxx7YTTzxxnpEAAADABFdcccXNrbVdB9o318HBiSeemD179swzEgAAAJigqv5Hb5+PKgAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdR8wz7DOfSU4+ed/HHv/45JnPTL72teQhD9m/5slPHm4335w89rH773/GM5Kf+7nkhhuS00/ff/9znpM8/OFD9tOfvv/+F7wgOfXU5KqrkrPO2n//S1+aPOAByQc/mDz/+fvvf+Urk5NOSt773uQlL9l//2tek9zjHsk73pG84hX773/DG5K73CV5y1uSV796//1ve1ty7LHJ618/3NZ717uSo45KXvWq5Pzz999/6aXD15e/PLnoon333e52ybvfPWy/+MXJJZfsu/9Od0ouuGDYft7zkg99aN/9J5yQvPGNw/ZZZw3fw7Xufvfk3HOH7TPPTD772X33n3TS8P1Lkic9Kbnxxn333//+ycteNmw/5jHJLbfsu/+UU5IXvnDYfvCDk69/fd/9D3tY8tznDtvr111i7Vl7w7a1t/9+a2/Ytvb232/tWXuJtWft7bvf2rP2EmvvUFx76811cAAAsGwuv274r8m/98IP57Ajv53/deUP5qvXHX/r/vvd9U6Lag0A5qJaa3ML2717d9uzZ8/c8gAAZnXi2e/ccP/15zx0Tp0AwM6pqitaa7sPtM85DgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACgy+AAAAAA6DI4AAAAALoMDgAAAIAugwMAAACga+rBQVUdXlUfraqLxvvHVNXFVXXt+PWOO9cmAAAAsAibecfBs5Ncs+b+2Ukuaa3dLckl430AAADgEDLV4KCqTkjy0CSvXfPwI5OcN26fl+RR29oZAAAAsHDTvuPglUn+ZZJvr3nsuNbaF5Jk/Pp9ByqsqjOrak9V7bnppptm6RUAAACYs4mDg6p6WJIvt9au2EpAa+3c1tru1truXbt2beUlAAAAgAU5YornPDDJI6rqIUlum+R7quqNSb5UVce31r5QVccn+fJONgoAAADM38R3HLTWntdaO6G1dmKSJyR5X2vtSUkuTHLG+LQzkrx9x7oEAAAAFmIzV1VY75wkD6qqa5M8aLwPAAAAHEKm+ajCrVprlya5dNy+Jckp298SAAAAcLCY5R0HAAAAwCHO4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgK6Jg4Oqum1VfbiqPlZVn6yq3xgfP6aqLq6qa8evd9z5dgEAAIB5muYdB99I8o9aaz+R5KQkp1XV/ZKcneSS1trdklwy3gcAAAAOIRMHB23wN+PdI8dbS/LIJOeNj5+X5FE70SAAAACwOFOd46CqDq+qq5J8OcnFrbU/TXJca+0LSTJ+/b5O7ZlVtaeq9tx0003b1DYAAAAwD1MNDlpr32qtnZTkhCT3qaofmzagtXZua213a233rl27ttgmAAAAsAibuqpCa+0rSS5NclqSL1XV8Ukyfv3ydjcHAAAALNY0V1XYVVXfO27fLsmpST6d5MIkZ4xPOyPJ23eoRwAAAGBBjpjiOccnOa+qDs8waDi/tXZRVX0oyflV9dQkn0/yuB3sEwAAAFiAiYOD1trHk9zrAI/fkuSUnWgKAAAAODhs6hwHAAAAwGoxOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoMvgAAAAAOgyOAAAAAC6DA4AAACALoMDAAAAoGvi4KCq7lJV76+qa6rqk1X17PHxY6rq4qq6dvx6x51vFwAAAJinad5x8M0kz2mt/WiS+yX5xaq6Z5Kzk1zSWrtbkkvG+wAAAMAhZOLgoLX2hdbaleP2/0pyTZI7J3lkkvPGp52X5FE71CMAAACwIJs6x0FVnZjkXkn+NMlxrbUvJMNwIcn3dWrOrKo9VbXnpptumrFdAAAAYJ6mHhxU1dFJLkhyVmvtr6eta62d21rb3VrbvWvXrq30CAAAACzIVIODqjoyw9DgTa21Pxof/lJVHT/uPz7Jl3emRQAAAGBRprmqQiX5/STXtNb+/ZpdFyY5Y9w+I8nbt789AAAAYJGOmOI5D0xyepKrq+qq8bHnJzknyflV9dQkn0/yuB3pEAAAAFiYiYOD1toHklRn9ynb2w4AAABwMNnUVRUAAACA1WJwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHQZHAAAAABdEwcHVfW6qvpyVX1izWPHVNXFVXXt+PWOO9smAAAAsAjTvOPg9UlOW/fY2Ukuaa3dLckl430AAADgEDNxcNBa+7+T/OW6hx+Z5Lxx+7wkj9retgAAAICDwVbPcXBca+0LSTJ+/b7tawkAAAA4WOz4yRGr6syq2lNVe2666aadjgMAAAC20VYHB1+qquOTZPz65d4TW2vnttZ2t9Z279q1a4txAAAAwCJsdXBwYZIzxu0zkrx9e9oBAAAADibTXI7xvyT5UJJ7VNWNVfXUJOckeVBVXZvkQeN9AAAA4BBzxKQntNae2Nl1yjb3AgAAABxkdvzkiAAAAMDyMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAuo5YdAMAh4oTz37nhvuvP+ehc+oEAAC2j3ccAAAAAF0GBwAAAECXwQEAAADQZXAAAAAAdBkcAAAAAF0GBwAAAECXyzECsLQ2ugSmy18CAGwP7zgAAAAAugwOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAugwOAAAAgC6DAwAAAKDriEU3AAAAB4sTz35nd9/15zx0jp0Aa230u5n4/dxp3nEAAAAAdBkcAAAAAF3VWptb2O7b377tufe9933w8Y9PnvnM5GtfSx7ykP2Lnvzk4XbzzcljH7v//mc8I/m5n0tuuCE5/fT99z/nOcnDH5585jPJ05++//4XvCA59dTkqquSs87af/9LX5o84AHJBz+YPP/5++9/5SuTk05K3vve5CUv2X//a16T3OMeyTvekbziFfvvf8MbkrvcJXnLW5JXv3r//W97W3LsscnrXz/c1nvXu5Kjjkpe9ark/PP333/ppcPXl788ueiifffd7nbJu989bL/4xckll+y7/053Si64YNh+3vOSD31o3/0nnJC88Y3D9llnDd/Dte5+9+Tcc4ftM89MPvvZffefdNLw/UuSJz0pufHGffff//7Jy142bD/mMcktt+y7/5RTkhe+cNh+8IOTr3993/0Pe1jy3OcO2yefnP1Ye9Zesq1r7/Lrhudd8sP3ye/d99FJkjf/4dm3lt3vrncaNqy9bVt7l5+zf/0Tfv6cJMn1x16zMmvvVo57O7L29v5uP/lx/zp/e+Rt86Qr35mHffq/37r/1t/tFTzu3eoQWnuXP2H/+t885cx86ri75vpTv2vhxz1/5x66a+9gOu7d6iBae5f/0b711x1z5zz/tF9Okrz0Pb+Tn//ev9233trb9Nqryy67orW2e/8ne8cBAAAAsIH5vuNg9+62Z8+eueUBzJOT9syfk5gxD363V4vjChycHIt3XlV5xwEAAACweQYHAAAAQJfBAQAAANBlcAAAAAB0GRwAAAAAXQYHAAAAQJfBAQAAANBlcAAAAAB0GRwAAAAAXQYHAAAAQJfBAQAAANBlcAAAAAB0GRwAAAAAXQYHAAAAQJfBAQAAANBlcAAAAAB0GRwAAAAAXUcsugEAWIQTz37nhvuvP+ehc+oEgEWY9e8Bf48wDwfLOjM4AFhxB8tfSAAAHJwMDgAA2I+hIgB7OccBAAAA0GVwAAAAAHQZHAAAAABdBgcAAABAl8EBAAAA0GVwAAAAAHS5HCMArBCX2AMANsvgAAAADgIbDfYM9Q7MMBTmw0cVAAAAgK6ZBgdVdVpVfaaq/qyqzt6upgAAAICDw5Y/qlBVhyf5v5I8KMmNST5SVRe21j61Xc3BsvP2ufmb5Xu+zD8vb29dLqu6Ttm8Zf15z9q335H58/fIcvHzmi/HldnOcXCfJH/WWrsuSarqzUkemeSQHxzs5F+Gq7Dolo0DBcDA8XC5+HnNn+85q2BZ/19mkb+fh8KxoVprWyusemyS01prTxvvn57kvq21X1r3vDOTnDnevUeSz2zwsscmuXlLDc1Wu8jsZe1b9mplL2vfslcre1n7lr1a2cvat+zlqpUtexlqZR982T/YWtt1wD2ttS3dkjwuyWvX3D89ye9s9fXG19iziNpFZi9r37JXK3tZ+5a9WtnL2rfs1cpe1r5lL1etbNnLUCt7ubJnOTnijUnusub+CUn+YobXAwAAAA4yswwOPpLkblX1Q1V1myRPSHLh9rQFAAAAHAy2fHLE1to3q+qXkvxxksOTvK619skZ+zl3QbWLzF7WvmWvVvay9i17tbKXtW/Zq5W9rH3LXq5a2bKXoVb2EmVv+eSIAAAAwKFvlo8qAAAAAIc4gwMAAACgy+AAAAAA6DI4AAAAALoOusFBVR29gMxjZqh9xCKyq+pHquoxVXXPKZ77vVvJWPcaR6zZPrqqdm+m96raVVX3qqp/sIifMQAAwKqpqh+uqu8at0+uqmdt5f8PD7rBQZJPbbRz/B/Py6vqhqo6t6ruuGbfhye9eFU9sKquqapPVtV9q+riJHvG17v/hNpHr7s9Jsm5e+9Pkf2CNdv3rKrPJrmiqq6vqvtOqH1/VR07bp+e5F1JHpzkLVX1yxOib66q91bVU7eySKrqyUm+VFWfraoHJ/l4kt9K8rGqeuKE2ntW1XuTfCjJnyZ5bZKrq+r1VXWHKfP/XlX9q6r6j1X1H8btH93sn2Pdaz5lE9mnrB92VNVpU9Tep6r+4bh9z6r6lap6yNY6TqrqP2+x7n8fs39myufft6q+Z9y+XVX9RlW9o6p+a9LPbDwQ3WWLfd6mqn6hqk4d7/98Vf1uVf1iVR055Wv8cFU9d1wnr6iqf77IdTa+7sS1Nss6G5+3bWttq+tsrJ16rc2yzsaaha21WdbZWO+YFse0KV/DMc0xbZo+HdMc03b8mDbWL2ytOabd+lpbWWcXJPlWVf1Ikt9P8kNJ/nDT2Yu4HGNV/UpvV5Jfa611/yW7qj6Q5CVJLk/ytCRPSfKI1trnquqjrbV7Tcj+cJKnJjk6yTuSPKq19oGq+skkv9Nae+AGtd9M8p4kXx57TZLHJnlbktZa+6cTsq9srf3kuP3OJL/bWnt3Vd0nyStbaw/YoPYTrbUfG7c/kuS01totVXVUkstbaz++Qe3VSZ6X5IlJTkvygST/JcnbW2tf36jnNfU/neT2ST6W5F7j9/u4JBdPyL48yRmttc+Mf85fbK2dUVX/LMnPttYeOyH7X419vznJjePDJyR5QpI3t9bOmdR/53U/31r7gQnPeVaSX0xyTZKTkjy7tfb2cd+tP8tO7YsyDHaOSHJxkvsmuTTJqUn+uLX2byZkX7j+oQw/g/clSWut+06Xqvpwa+0+4/Y/G/8M/zXJzyR5x6TvWVV9MslPtNa+WVXnJvlahjV+yvh4d0hWVX+V5KtJPpdhjb21tXbTRnlrat+U4ft1VJKvZPgd/aMxt1prZ0yof1aShye5LMlDklyV5H8m+cdJntlau3SD2h1ZZ+Nrb7jWZlln43O2vNZmWWdj/ZbX2izrbKxfyFqbZZ2N9Y5p40NxTHNMO3C9Y5pjmmPaxtkLOaaN9QtZa45p33komzimrXmdK1trP1lVv5rkb1trvzPN/zfvp7U291uSv03y4iQvOsDtKxNqr1p3/6eTXJvkfkmunCL7o2u2r1m3b8P6JP8wySVJnpHvDF3+fBN/7isP1MeB7h+o7yR3Hrffn+S24/bhST65idzbJXl8hl/yW5L84RR9X7Vm+y/W7fv4hNqPbdDLp6bI/mySIw/w+G2SXDuh9uOd29VJvjFF9tVJjh63T0yyJ8PBYpqf19Xjz+aoJH+d5HvWfP83/J7t/T4leWOSk5P81Pj1C+P2T01aK2u2P5Jk17j93UmuniL7mrV99NbCBuv0sAx/+f1+kpsyDNvOSHL7ST+v8esRSb6U5PDxfk35Pbt6Tc1RSS4dt39gip/XltfZrGttlnU261qbZZ3NutZmWWeLXGuzrLNZ19os62zWtTbLOpt1rc2yzmZda4taZ7OutVnW2axrbZZ1Nutam2WdzbrWZllni1xrs6yzWdfaLOts1rU2yzqbda3Nss5mXWuzrLNFrrVZ1tmsa22WdTbrWptlna17nT/NMHj5RJIfGh/7xLT1e2+3fm59zq5M8t9aa1es31FVT5tQW1V1h9baXyVJa+39NXxk4IIk03zmfu3HM563bt9tNipsrX2kqh6U5JeTvG+cfrUpMve66zg5qiQnVNVRrbWvjfsmvb3nXyT5k6q6IMknx/z3JPk/kvzBhNq9745IG95hcH6S88e3Bj1qir4/X1Uvy/COg09X1SsyDB5OzbB4N/K5qnphhoHLozNMF1PD25mmWX/fTvL9Sf7HusePH/dt5LgkP5thmrlWJfngFNmHt9b+Jklaa9dX1clJ3lZVP5g139OOb7bWvpXka1X1udbaX4+v8/WqmtR3kuxO8uwkv5bkV1trV1XV11trl01Re1gNH+E5LMOA66Yx+6vju2Ym+URVPaW19gcZPo6yu7W2p6runuTvJtS21tq3k/xJhvV6ZIYp6xOTvDzJrgl93ybDX5xHJblDkr9M8l2Z/Pux1xFJvjXW3H5s6PM1+e1zs6yzZLa1Nss6S2Zba7Oss2S2tTbLOhtjFrbWtrrOEsc0xzTHtEkc0xzTHNM2tqhj2t7eF7HWHNO2dkzb6ylJ/nmSf9Na+/Oq+qEMA4lNWdTg4CkZ/rX7QHZPqP2tJD+a4aMKSZLW2ser6pQkL5wi+4V7/4e9tfbf9j5YVT+cZOJnRsZftv9QVW9N8sop8tZ65Lr7h43ZxyV59YTcS6vqAUl+PsMv2hVJvpHkl1trn56Q+6bOa/5VkvOm6PtJGd6i85UkZ2f4xXtehl/eJ0+o/adJnj8+/+MZFn8yHGx+YYrss5JcUlXXJrlhfOwHkvxIkl+aUHtRhgnhVet3VNWlU2R/sapO2lvfWvubqnpYktcl+QcTav+/NYOhe6/JvUOmOMCN6+y3x3X221X1pUz/+3qHDOujkrSq+t9aa1+s4XNZ0xzgnpZhjb8gyc1JPlRVN2T4/k8c7K37c/xdkguTXFhVt5tQ+/tJPp1hKvtrSd5aVddleDfRm6fo+7VJPlLDx2P+zwzHilTVrgx/qW3krGx9nSWzrbVZ1lkyw1qbcZ0ls621WdZZ1r/+HNfaLOsscUxzTHNMm8QxLY5pjmkbWtQxLVncWjsrjmlbOabtfZ1PJXnWmHvHDO8u2fTHOxZyjoMkqarDk5zTWvvVedauavZB0Pdvtdaeu8Xsw5LcJ8mdMxzwbkzykXF6t2Oq6oQMU8IvHmDfA1tr/88Gtd/VWvvGAR4/NsnxrbWrN9nLQ5M8sLX2/M3UrXuNo5Ic11r78ymff/skd81wgLqxtfalKWru3lr77Aw9fn+StNb+ooYTeZ6a5POttYknPh3r/36GweInphiora9dunU2Pmfb1tp2rLPxdaZea1tZZ2PdwtbaLOtsrF+6teaYtuUeHdP23eeY1q9zTNt87sF0THtYkgcc6se08TUWstaWcZ2Nz1n4MW0cjjwiw1q5KsNHVC5rrfXOO3jg11nU4CBJqup9SU5pW2hiltqDIbu19o8WUZvFfc8uaa2dspXcDV7z6Da+dWietbLnn73IvmGnVdUxrbVp/mVvW2tlzz97wX0/orW2/kRbc6mXvVzZ29D3sv6OyF6S7BquDvATGc73sOEV+ba7fhmzazwRYg2nBLhLa+1FVfXxtsEJ7g/ksMlP2VEfTfL2qjq91lzmcA61C8+uqgu3mj1L7ax9z1B/1Qx992z6l3WbamUvV+3E+qr68ZrtMq9brq/ZLzG7rNmzfs+33PuC+57lksBbrpU9/+wF973+8tGPzuYuH73ly0/PUrsD2Yv8cy/F93wbfl6zXGp8y7UHafaeFf1z73h2zXZZ+pnqlzl7jSOq6vgMJ8i/aBN1+77IVgu3yTEZznWw9l/QW4YT7+1k7apmL13ftfGlO4/u7Ju5Vvb8sxfZd5JXJfnX+c5lXj9Qw7+4fC7TnexnlvpXr2j2rN/zWXpfZN+/neEv7qOTvDPrLgmcpHtJ4BlrZc8/e5F9n5/9Lx/93RkuhTbN39uz1MteruxZ+350hsukJ8m/y3C2+VsvNZ6ke6nxGWtlr1b2rtbazeP2s5Lcv625LH2GY+JGZqlf5uy9fjPJHyf5QBtO9n/XDFcl3JRFDw5eu/5zIVU16S/S7ahd1exl7PulGQ4uBzrL7KR3zMxSK3v+2Yvs++jW2nvG7ZdX1RVJ3jNOd6f5aM4s9bKXK3vWvo9s4+cZq+qm1toHkqS1dmVNPjHVLLWy55+9yL7vn+ScDJd6+0+ttVZVJ7fWnjJF7qz1spcre9a+1/r+1tq7k6S19uEp1/l21Mo+9LP/rqru3Fr7f5P8TZKvjo9/I8OJGieZpX6Zs5MkrbW3JnnrmvvXJXnMtPVrX2hht6y79mjvse2uXdXsZew7wyVS7t3Zd8NO1cqef/aC+/5Ykjuse+zHM0xjb5kie8v1spcrezv6XrP9qHX7Nrym8iy1suefvci+x+ccluFKRu/PcEKx6ybVbFe97OXKnrH2KxnOyv+ODCdcO2radTpLrezVyk5ycobL0f9mkt/N8N98v57k4iTPnaLvLdcvc/aa17lthivkvSrD1SBel+R109bvvS3kHQc1fDbvAUl2rXt78fdkwvRkltpVzV7WvkezXLpzllrZ889eZN+zXuZ1lnrZy5W9yEsCz3Q5Ydlzz15k32mzXT56pnrZy5U9Y9+PXHf/sCSpKS41PmOt7BXKbrNdln6m+mXOXuMNGS6j+bMZhhD/JMk1m6i/taG535L8VJIXJfnC+HXv7VeS3G2nalc1e1n7XvMahyf5d1tca1uulT3/7GXtW/ZqZS9r37KtFdmyD6W+Za9W9rL2vejs8TU+On79+Pj1yCTv2+zrLOQdB621y5JcVlVfb63927X7qupx2eBkDbPUrmr2sva95jW+VVX3rqpq42qf1iy1suefvax9y16t7GXtW7a1Ilv2dteurd9s3ay1slcre1n7XnT26O/Gr1+pqh9L8sUkJ272RWoLx4dtU1VXttZ+ctJj2127qtnL2vf43FckuVuGE3vsPTFIWmsTr+gwS63s+Wcva9+yVyt7WfuWba3Iln0o9S17tbKXte+DIPtpSS7IcE6mP8hwtZ5fb639p2nq91rUOQ4enOQhSe5cVf9xza7b5zsTkW2vXdXsZe17nWOyQpehXPHsZe1b9mplL2vfsq0V2bK3u1a27HllL2vfC81urb123LwsyV2nqTmQRV2O8S8ynNzhEePXvX4wydd2sHZVs5e177VW7TKUq5y9rH3LXq3sZe1b9vyzl7Vv2auVvax9y16t7GXteyHZte+J6ffTWvv3U+bfWrCwW4YTM5yU5N8muT7DZWB+aadrVzV7Wfse61fqMpSrnL2sfcterexl7Vu2tSJb9qHUt+zVyl7WvheVne+clP7Xs+9J6l+U4aMKU/W+97aojyrcPckTkjwxw9su3pKkWms/vZO1q5q9rH2P9St5GcpVzF7WvmWvVvay9i3bWpEt+1DqW/ZqZS9r34vObq39xvg65yV5dmvtK+P9OyZ5xaT69Rb1UYVPJ/nvSR7eWvuzJKmqfzGH2lXNXta+k+Q2GU7gcUSG8yLs9ddJHruDtbLnn72sfcterexl7Vu2tSJb9nbXypY9r+xl7XvR2Xv9+N6hQZK01v5nVd1rE/W3Fs79luQfZ/iX5xuS/F6SU5L8+U7Xrmr2sva97nX+5QEee9xO18qef/ay9i17tbKXtW/Z1ops2YdS37JXK3tZ+z4Isj+W5I5r7h+T5Opp62+t22zBdt6SfHeSf5Lkogwnynt1kp/Z6dpVzV7Wvsf6lfpM0ipnL2vfslcre1n7lm2tyJZ9KPUte7Wyl7XvgyD7F5Jck+TFSX4zwzvCT5+2fu9tUR9VSJK01r6a5E1J3lRVxyR5XJKzk/zJTtauavYy9l0rehnKVcxe1r5lr1b2svYt21qRLftQ6lv2amUva9+Lzt6rtfafq2pPhss5VpJHt9Y+NW39XgsdHKzVWvvLJK8Zb3OrXdXsJep7VS9DuYrZy9q37NXKXta+ZVsrsmVvd61s2fPKXta+F519q3FQsOlhwfoXcXM76G9Z3ctQrlz2svYte7Wyl7Vv2daKbNmHUt+yVyt7WftedPZ23Q6adxzAgdTqXoZy5bKXtW/Zq5W9rH3LtlZkyz6U+pa9WtnL2veis7fdvCcVbm6buSX5dpLLkvzImseu2+la2fPPXta+Za9W9rL2LdtakS37UOpb9mplL2vfi87e7tthgYPbY5J8Mcn7q+r3quqUDCf12Ola2fPPXta+Za9W9rL2LdtakS37UOpb9mplL2vfi87eXouaWLi5beaW1b0M5cplL2vfslcre1n7lm2tyJZ9KPUte7Wyl7XvRWdv122uYW5u23FLckySpyd53zxrZc8/e1n7lr1a2cvat2xrRbbsQ6lv2auVvax9Lzp7lluNDQAAAADsxzkOAAAAgC6DAwAAAKDL4AAAAADoMjgAAAAAuv5/hHMvxs60A4sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (18,6))\n",
    "miss_per = ((data.isnull().sum())*100/len(data[\"Attr1\"]))\n",
    "\n",
    "miss_per.plot(kind=\"bar\")\n",
    "                \n",
    "ax = plt.axhline(y=5,color='r',linestyle = '--')\n",
    "ax = plt.axhline(y=45,color='b',linestyle = '--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7001196",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{This bar graph shows that we have 5 Attributes with missing values more than 5% and for Attr37} \\\\ \\textit{it is 45% which is very high, we should delete Attr37 and fill the missing values with median,} \\\\ \\textit{this we will do based on the basis of class}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339831c",
   "metadata": {},
   "source": [
    "*$\\color{red}{\\text{Method}}$: Replace the missing values with median of class and then replace the outliers with lower and upper values of IQR range.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f72406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy(deep=True)\n",
    "df = df.drop(columns=[\"Attr37\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b104b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class0 = df[df[\"class\"] == \"b'0'\"]                    # Seperating the data on the basis of class\n",
    "df_class1 = df[df[\"class\"] == \"b'1'\"]\n",
    "\n",
    "df_class0 = df_class0.drop('class', axis = 1)\n",
    "df_class1 = df_class1.drop('class', axis = 1)\n",
    "\n",
    "df_class0 = df_class0.fillna(df_class0.median())       # Filling the missing values with median of class = b'0' (Non-Bankruptcy)\n",
    "df_class1 = df_class1.fillna(df_class1.median())       # Filling the missing values with median of class = b'1' (Bankruptcy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e96c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class0_base = df_class0.copy(deep=True)\n",
    "df_class1_base = df_class1.copy(deep=True)\n",
    "df_class0_base[\"class\"] = 0\n",
    "df_class1_base[\"class\"] = 1\n",
    "data_base = pd.concat([df_class0_base,df_class1_base])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9de9b",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{data_base:}}$*This dataset has no missing values but it contains all outliers intact*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54fad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Attributes  Skew_Values\n",
      "0       Attr1    51.971750\n",
      "1       Attr2    71.596019\n",
      "2       Attr3   -71.691576\n",
      "3       Attr4   101.392697\n",
      "4       Attr5   -96.973066\n",
      "5       Attr6   -69.367311\n",
      "6       Attr7    51.122493\n",
      "7       Attr8    57.629129\n",
      "8       Attr9    89.251692\n",
      "9      Attr10   -71.532339\n",
      "10     Attr11    50.325599\n",
      "11     Attr12    63.855413\n",
      "12     Attr13    95.595825\n",
      "13     Attr14    51.122608\n",
      "14     Attr15    78.046506\n",
      "15     Attr16    66.222372\n",
      "16     Attr17    56.901407\n",
      "17     Attr18    49.391027\n",
      "18     Attr19   -56.105443\n",
      "19     Attr20    72.157172\n",
      "20     Attr21   100.453767\n",
      "21     Attr22    49.436071\n",
      "22     Attr23   -56.870025\n",
      "23     Attr24    59.385911\n",
      "24     Attr25   -70.658843\n",
      "25     Attr26    68.129584\n",
      "26     Attr27    60.679954\n",
      "27     Attr28    40.724742\n",
      "28     Attr29    -0.030266\n",
      "29     Attr30   -58.811629\n",
      "30     Attr31   -57.022819\n",
      "31     Attr32    91.538164\n",
      "32     Attr33    50.013503\n",
      "33     Attr34    75.125077\n",
      "34     Attr35    52.688413\n",
      "35     Attr36    31.606763\n",
      "36     Attr38   -71.684939\n",
      "37     Attr39   -52.690921\n",
      "38     Attr40    53.795515\n",
      "39     Attr41   102.473398\n",
      "40     Attr42   -57.960029\n",
      "41     Attr43    61.682816\n",
      "42     Attr44    78.452008\n",
      "43     Attr45    40.549943\n",
      "44     Attr46   101.471037\n",
      "45     Attr47    74.792720\n",
      "46     Attr48    38.217771\n",
      "47     Attr49   -60.878162\n",
      "48     Attr50   101.998964\n",
      "49     Attr51    71.855879\n",
      "50     Attr52    96.137036\n",
      "51     Attr53    44.702538\n",
      "52     Attr54    41.873951\n",
      "53     Attr55    29.098658\n",
      "54     Attr56  -100.765691\n",
      "55     Attr57   -63.746132\n",
      "56     Attr58    86.114674\n",
      "57     Attr59    92.568026\n",
      "58     Attr60    99.391703\n",
      "59     Attr61    37.169595\n",
      "60     Attr62   -61.864452\n",
      "61     Attr63    36.083490\n",
      "62     Attr64    32.319484\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1728x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting skewness for data_base- dataFrame with outliers intact\n",
    "variables = [c for c in data_base.columns if c!=\"class\"]\n",
    "skew = []\n",
    "\n",
    "for variable in variables:\n",
    "    skew.append([variable,data_base[variable].skew()])\n",
    "print(pd.DataFrame(skew,columns=[\"Attributes\",\"Skew_Values\"]))\n",
    "\n",
    "skew = pd.DataFrame(skew,columns=['Attributes','skewness'])\n",
    "\n",
    "fig,ax  = plt.subplots(1,1,figsize=(24,8))\n",
    "axes = skew.plot(kind = 'bar',figsize=(24,8),ax=ax,\n",
    "                 xlabel = (\"Attributes (Attr(i-1)\"),\n",
    "                 ylabel = (\"Skewness\"),\n",
    "                 title = (\"Skewness Distribution of Attributes\"))\n",
    "axes = plt.axhline(y=1,color='g',linestyle = '--')\n",
    "axes = plt.axhline(y=-1,color='g',linestyle = '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442676c",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textbf{As seen above, if the outliers are left intact, the skewness values are astronomically high, therefore treatment of outliers seem necessary}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7d130",
   "metadata": {},
   "source": [
    "## *Baseline: Feature Selection and Training for data with outliers intact*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965931ac",
   "metadata": {},
   "source": [
    "* Logistic Regression Classifier \n",
    "* Random forest classifier with weights \n",
    "\n",
    "**2 methods used for outlier intact data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e186c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_fp = data_base.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1485aa08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3dc8fc0fd104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_base_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_base_fp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x = data_base_fp.drop('class', axis=1)\n",
    "y = data_base_fp['class']\n",
    "trainX, testX, trainY, testY = train_test_split(x, y,stratify=y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fef7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(trainX)\n",
    "\n",
    "logistic = LogisticRegression(C=0.1, penalty='l1', solver='liblinear')\n",
    "logistic.fit(trainX,trainY)\n",
    "\n",
    "sel_ = SelectFromModel(logistic,threshold=0.03)\n",
    "sel_.fit(scaler.transform(trainX.fillna(0)), trainY)\n",
    "\n",
    "selected_feature = trainX.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((trainX.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feature)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))\n",
    "\n",
    "selected_features = trainX.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\n",
    "selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf85277",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(testY, logistic.predict(testX), average='macro')\n",
    "recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features selected via logistic regression\n",
    "features_o=['Attr6', 'Attr10', 'Attr24', 'Attr29', 'Attr31', 'Attr33', 'Attr35',\n",
    "       'Attr48', 'Attr55', 'Attr61']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1213b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with class_weight to check if we can improve the recall metric\n",
    "df_model = data_base.copy(deep=True)\n",
    "print(df_model.shape)\n",
    "\n",
    "y = df_model['class']\n",
    "x = df_model.drop('class', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "data_results = []\n",
    "\n",
    "for weight in range(1,20):\n",
    "    forest = RandomForestClassifier(n_estimators = 100, max_features = 40, max_depth = 40, class_weight={0:1,1:weight}, random_state = 42)\n",
    "    forest.fit(X_train,y_train)\n",
    "    model = SelectFromModel(forest, prefit=True)\n",
    "    recall_ = recall_score(y_test, forest.predict(X_test), average='macro')\n",
    "    names = model.get_support()\n",
    "    data_results.append([weight, model.transform(X_train).shape[1], recall_.round(decimals=2), X_train.columns[names].values])\n",
    "\n",
    "    \n",
    "results = pd.DataFrame(data_results, columns=['weight','predictors','recall','predictor_names'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57eb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features selected via Random forest method with weights for bankrupt class\n",
    "feature_14= ['Attr5', 'Attr15', 'Attr16', 'Attr21', 'Attr24', 'Attr26',\n",
    "        'Attr27', 'Attr29', 'Attr34', 'Attr40', 'Attr45', 'Attr46',\n",
    "        'Attr55', 'Attr58']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(([0.5032,10],[0.65,14]),\n",
    "             index=['Logistic','Random Forest'],\n",
    "            columns=['Recall Scores','Optimal number of Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce00ba6",
   "metadata": {},
   "source": [
    "### $\\color{black}{\\textbf{After feature selection, the base line model of Logistic Regression and Random Forest yield}} \\\\ \\color{red}{\\textbf{a meagre recall value of only 0.5032 and 0.65 respectively on the test data - which is poor}}$\n",
    "\n",
    "$\\color{red}{\\textbf{Therefore, we look at an outlier treatment}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c029f8b",
   "metadata": {},
   "source": [
    "### $\\color{green}{\\textbf{Outlier Method used:}}$\n",
    "\n",
    "$\\color{green}{\\text{Due to the nature of the data, which is characterised by high skewness presumably because of the} \\\\ \\text{diversity of scale and financial status of many different kind of companies, we decided to preserve} \\\\ \\text{this nature  when treating outliers. By clipping the magnitude of the outliers,(similar to winsorization)} \\\\ \\text{rather than suppressing their existence(with a median),we tried to keep the ability of the different}\\\\ \\text{predictors to describe the amount of diversity of companies within the dataset}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f097a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for treating outliers\n",
    "def Remove_Outliers(data):\n",
    "    variables = [c for c in data.columns if c!=\"class\"]\n",
    "\n",
    "    for variable in variables:\n",
    "        Q1 = data[variable].quantile([0.25]).values[0]\n",
    "        Q3 = data[variable].quantile([0.75]).values[0]\n",
    "        IQR = [Q1 - 1.5*(Q3-Q1) ,Q3 + 1.5*(Q3-Q1) ]\n",
    "        data.loc[data[variable] > IQR[1],variable] = IQR[1]\n",
    "        data.loc[data[variable] < IQR[0],variable] = IQR[0]\n",
    "    return data\n",
    "\n",
    "df_class0 = Remove_Outliers(df_class0)\n",
    "df_class1 = Remove_Outliers(df_class1)\n",
    "\n",
    "df_class0[\"class\"] = 0\n",
    "df_class1[\"class\"] = 1\n",
    "\n",
    "Bdata = pd.concat([df_class0,df_class1])\n",
    "Bdata.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first find the skewness in dataset and plot the bar graph indicationg the distribution.\n",
    "\n",
    "variables = [c for c in Bdata.columns if c!=\"class\"]\n",
    "skew = []\n",
    "\n",
    "for variable in variables:\n",
    "    skew.append([variable,Bdata[variable].skew()])\n",
    "print(pd.DataFrame(skew,columns=[\"Attributes\",\"Skew_Values\"]))\n",
    "\n",
    "skew = pd.DataFrame(skew,columns=['Attributes','skewness'])\n",
    "\n",
    "fig,ax  = plt.subplots(1,1,figsize=(24,8))\n",
    "axes = skew.plot(kind = 'bar',figsize=(24,8),ax=ax,\n",
    "                 xlabel = (\"Attributes (Attr(i-1)\"),\n",
    "                 ylabel = (\"Skewness\"),\n",
    "                 title = (\"Skewness Distribution of Attributes\"))\n",
    "axes = plt.axhline(y=1,color='g',linestyle = '--')\n",
    "axes = plt.axhline(y=-1,color='g',linestyle = '--')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b01f5",
   "metadata": {},
   "source": [
    "*$\\color{red}{\\textit{This Data contails skewness, which we need to treat beofre model building, we can accept skewness} \\\\ \\textit{of -1.0 to +1.0 but there are 15 features for which skewness are greater than +1.0.}}$*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa0fb5",
   "metadata": {},
   "source": [
    "## *2. Data Visualization and Exploration*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3241fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dict, which we will use to retrive the actual names of attributes.\n",
    "\n",
    "names = {  \"Attr1\" : \"net profit / total assets\" ,\n",
    " \"Attr2\" : \"total liabilities / total assets\" ,\n",
    " \"Attr3\" : \"working capital / total assets\" ,\n",
    " \"Attr4\" : \"current assets / short-term liabilities\" ,\n",
    " \"Attr5\" : \"[(cash + short-term securities + receivables - short-term liabilities) / (operating expenses - depreciation)] * 365\" ,\n",
    " \"Attr6\" : \"retained earnings / total assets\" ,\n",
    " \"Attr7\" : \"EBIT / total assets\" ,\n",
    " \"Attr8\" : \"book value of equity / total liabilities\" ,\n",
    " \"Attr9\" : \"sales / total assets\" ,\n",
    " \"Attr10\" : \"equity / total assets\" ,\n",
    " \"Attr11\" : \"(gross profit + extraordinary items + financial expenses) / total assets\" ,\n",
    " \"Attr12\" : \"gross profit / short-term liabilities\" ,\n",
    " \"Attr13\" : \"(gross profit + depreciation) / sales\" ,\n",
    " \"Attr14\" : \"(gross profit + interest) / total assets\" ,\n",
    " \"Attr15\" : \"(total liabilities * 365) / (gross profit + depreciation)\" ,\n",
    " \"Attr16\" : \"(gross profit + depreciation) / total liabilities\" ,\n",
    " \"Attr17\" : \"total assets / total liabilities\" ,\n",
    " \"Attr18\" : \"gross profit / total assets\" ,\n",
    " \"Attr19\" : \"gross profit / sales\" ,\n",
    " \"Attr20\" : \"(inventory * 365) / sales\" ,\n",
    " \"Attr21\" : \"sales (n) / sales (n-1)\" ,\n",
    " \"Attr22\" : \"profit on operating activities / total assets\" ,\n",
    " \"Attr23\" : \"net profit / sales\" ,\n",
    " \"Attr24\" : \"gross profit (in 3 years) / total assets\" ,\n",
    " \"Attr25\" : \"(equity - share capital) / total assets\" ,\n",
    " \"Attr26\" : \"(net profit + depreciation) / total liabilities\" ,\n",
    " \"Attr27\" : \"profit on operating activities / financial expenses\" ,\n",
    " \"Attr28\" : \"working capital / fixed assets\" ,\n",
    " \"Attr29\" : \"logarithm of total assets\" ,\n",
    " \"Attr30\" : \"(total liabilities - cash) / sales\" ,\n",
    " \"Attr31\" : \"(gross profit + interest) / sales\" ,\n",
    " \"Attr32\" : \"(current liabilities * 365) / cost of products sold\" ,\n",
    " \"Attr33\" : \"operating expenses / short-term liabilities\" ,\n",
    " \"Attr34\" : \"operating expenses / total liabilities\" ,\n",
    " \"Attr35\" : \"profit on sales / total assets\" ,\n",
    " \"Attr36\" : \"total sales / total assets\" ,\n",
    " \"Attr37\" : \"(current assets - inventories) / long-term liabilities\" ,\n",
    " \"Attr38\" : \"constant capital / total assets\" ,\n",
    " \"Attr39\" : \"profit on sales / sales\" ,\n",
    " \"Attr40\" : \"(current assets - inventory - receivables) / short-term liabilities\" ,\n",
    " \"Attr41\" : \"total liabilities / ((profit on operating activities + depreciation) * (12/365))\" ,\n",
    " \"Attr42\" : \"profit on operating activities / sales\" ,\n",
    " \"Attr43\" : \"rotation receivables + inventory turnover in days\" ,\n",
    " \"Attr44\" : \"(receivables * 365) / sales\" ,\n",
    " \"Attr45\" : \"net profit / inventory\" ,\n",
    " \"Attr46\" : \"(current assets - inventory) / short-term liabilities\" ,\n",
    " \"Attr47\" : \"(inventory * 365) / cost of products sold\" ,\n",
    " \"Attr48\" : \"EBITDA (profit on operating activities - depreciation) / total assets\" ,\n",
    " \"Attr49\" : \"EBITDA (profit on operating activities - depreciation) / sales\" ,\n",
    " \"Attr50\" : \"current assets / total liabilities\" ,\n",
    " \"Attr51\" : \"short-term liabilities / total assets\" ,\n",
    " \"Attr52\" : \"(short-term liabilities * 365) / cost of products sold)\" ,\n",
    " \"Attr53\" : \"equity / fixed assets\" ,\n",
    " \"Attr54\" : \"constant capital / fixed assets\" ,\n",
    " \"Attr55\" : \"working capital\" ,\n",
    " \"Attr56\" : \"(sales - cost of products sold) / sales\" ,\n",
    " \"Attr57\" : \"(current assets - inventory - short-term liabilities) / (sales - gross profit - depreciation)\" ,\n",
    " \"Attr58\" : \"total costs /total sales\" ,\n",
    " \"Attr59\" : \"long-term liabilities / equity\" ,\n",
    " \"Attr60\" : \"sales / inventory\" ,\n",
    " \"Attr61\" : \"sales / receivables\" ,\n",
    " \"Attr62\" : \"(short-term liabilities *365) / sales\" ,\n",
    " \"Attr63\" : \"sales / short-term liabilities\" ,\n",
    " \"Attr64\" : \"sales / fixed assets\" ,\n",
    " \"class\" : \"class\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f0de1",
   "metadata": {},
   "source": [
    "### *Correlation Analysis,plotting highly correlated varibales to see the pattern and choose the attributes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = Bdata.corr()\n",
    "cor_list = []\n",
    "for i in cor.columns:\n",
    "    cor_list.append(cor[(cor[i] > 0.95)|(cor[i]< -0.85)][i])\n",
    "    \n",
    "cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax  = plt.subplots(1,1,figsize = (28,20))\n",
    "sns.heatmap(data= cor[(cor>0.95) | (cor<-.85)],vmin=-1,vmax=1,cmap=\"PiYG\",linewidths=0.8,ax=ax,annot=True)\n",
    "ax.set_title('Collinearities (Correlations > 0.95 or < -0.85)', fontsize=20)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c1db5a",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{Heatmap shows that data contains features which are highly correlated, we plot boxplot, pairplot to decide which variables to take into consideration.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff90647",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,5,figsize = (36,8))\n",
    "set1 = ['Attr1','Attr7','Attr11','Attr14','Attr18',\"class\"]\n",
    "\n",
    "for i in range(len(set1)-1):\n",
    "    sns.boxplot(data=Bdata,y=set1[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set1[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set1[i]],fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "col = dict(([i,names[i]] for i in set1))\n",
    "ag = sns.pairplot(data=Bdata[set1].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(20,20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c61ee9",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr1','Attr7','Attr11','Attr14','Attr18'} are highly correlated, we will take Attr11, which is more correlated with Attr1,Attr7 Attr14 and Attr18.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40235d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize = (36,10))\n",
    "set2 = ['Attr2','Attr10',\"class\"]\n",
    "\n",
    "for i in range(len(set2)-1):\n",
    "    sns.boxplot(data=Bdata,y=set2[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set2[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set2[i]],fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e4970",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dict(([i,names[i]] for i in set2))\n",
    "ag = sns.pairplot(data=Bdata[set2].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(12,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e691a56",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr2','Attr10'} are nagative correlated, we will take Attr10}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d75428",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize = (36,10))\n",
    "set3 = ['Attr8','Attr17',\"class\"]\n",
    "\n",
    "for i in range(len(set3)-1):\n",
    "    sns.boxplot(data=Bdata,y=set3[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set3[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set3[i]],fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dict(([i,names[i]] for i in set3))\n",
    "ag = sns.pairplot(data=Bdata[set3].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(10,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23555d3",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr8','Attr17'} are correlated, we will take Attr17.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be70de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3,figsize = (36,10))\n",
    "set4 = ['Attr19','Attr23','Attr31',\"class\"]\n",
    "\n",
    "for i in range(len(set4)-1):\n",
    "    sns.boxplot(data=Bdata,y=set4[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set4[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set4[i]],fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dict(([i,names[i]] for i in set4))\n",
    "ag = sns.pairplot(data=Bdata[set4].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(10,10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674f9f0",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr19','Attr23','Attr31'} are positive correlated, we will take Attr23.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize = (36,10))\n",
    "set5 = ['Attr16','Attr26',\"class\"]\n",
    "\n",
    "for i in range(len(set5)-1):\n",
    "    sns.boxplot(data=Bdata,y=set5[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set5[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set5[i]],fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dict(([i,names[i]] for i in set5))\n",
    "ag = sns.pairplot(data=Bdata[set5].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(10,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad664d",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr16','Attr26'} are positive correlated, we will take Attr16.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0230fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize = (36,10))\n",
    "set6 = ['Attr56','Attr58',\"class\"]\n",
    "\n",
    "for i in range(len(set6)-1):\n",
    "    sns.boxplot(data=Bdata,y=set5[i],x= \"class\",width=0.75,ax=axes[i])\n",
    "    axes[i].set_ylabel(set6[i])\n",
    "    axes[i].set_xlabel(\"class\")\n",
    "    axes[i].set_title(names[set6[i]],fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dict(([i,names[i]] for i in set6))\n",
    "ag = sns.pairplot(data=Bdata[set6].rename(columns=col),hue=\"class\",markers=['s','D'],diag_kind='hist')\n",
    "ag.map_upper(sns.scatterplot)\n",
    "ag.fig.set_size_inches(10,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23053af",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{These variables {'Attr56','Attr58'} are negative correlated, we will take Attr58.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f1e901",
   "metadata": {},
   "source": [
    "### $\\color{green}{\\textit{Based on correlations of Attributes, we can choose 52 features on which we can do our analysis.} \\\\\\textit {These are below:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_prob = ['Attr3', 'Attr4', 'Attr5', 'Attr6','Attr9', 'Attr10', 'Attr11', 'Attr12', 'Attr13', 'Attr15', 'Attr17', \n",
    "                 'Attr19', 'Attr20', 'Attr21', 'Attr22','Attr23', 'Attr24', 'Attr25', 'Attr26', 'Attr27', 'Attr28', 'Attr29',\n",
    "                 'Attr30', 'Attr31','Attr33', 'Attr34', 'Attr35', 'Attr36','Attr38', 'Attr39', 'Attr40', 'Attr41', 'Attr42',\n",
    "                 'Attr43', 'Attr44','Attr45', 'Attr46', 'Attr47', 'Attr48', 'Attr49', 'Attr50','Attr52', 'Attr53','Attr55',\n",
    "                 'Attr57', 'Attr58','Attr59', 'Attr60', 'Attr61', 'Attr62', 'Attr63', 'Attr64', 'class'] \n",
    "len(features_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6247db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold,cross_val_score,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import recall_score,confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc97f201",
   "metadata": {},
   "source": [
    "# *3. Feature Selection after Outlier Treatment*\n",
    "\n",
    "For feature selection we will use \n",
    "1. LogisticRegression with Lasso.\n",
    "2. RandomForestCLassifier with variable class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1f74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6d0141",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bdata_fp = Bdata[features_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bdata_fp.drop('class', axis=1)\n",
    "y = Bdata_fp['class']\n",
    "trainX, testX, trainY, testY = train_test_split(x, y,stratify=y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdebced",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(trainX)\n",
    "\n",
    "logistic = LogisticRegression(C=0.1, penalty='l1', solver='liblinear')\n",
    "logistic.fit(trainX,trainY)\n",
    "\n",
    "sel_ = SelectFromModel(logistic,threshold=0.2)\n",
    "sel_.fit(scaler.transform(trainX.fillna(0)), trainY)\n",
    "\n",
    "selected_feature = trainX.columns[(sel_.get_support())]\n",
    "print('total features: {}'.format((trainX.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feature)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "      np.sum(sel_.estimator_.coef_ == 0)))\n",
    "\n",
    "selected_features = trainX.columns[(sel_.estimator_.coef_ != 0).ravel().tolist()]\n",
    "selected_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(testY, logistic.predict(testX), average='macro')\n",
    "recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4338066",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_log = ['Attr3', 'Attr6', 'Attr10', 'Attr17', 'Attr23', 'Attr33', 'Attr35',\n",
    "       'Attr38', 'Attr39', 'Attr46', 'Attr48', 'Attr52', 'Attr58', 'Attr59',\n",
    "       'Attr61', 'Attr62','class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d586cc",
   "metadata": {},
   "source": [
    "### $\\color{green}{\\textit{Using RandomForestClassifiers Feature Importance to select the features:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65906ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100, max_features = 40, max_depth = 40, random_state = 42)\n",
    "forest.fit(trainX,trainY)\n",
    "\n",
    "labels_ = features_prob[:-1]\n",
    "import_     = forest.feature_importances_\n",
    "\n",
    "important_features_dict = {}\n",
    "for idx, val in zip(labels_,import_):\n",
    "    important_features_dict[idx] = val\n",
    "\n",
    "important_features_dict\n",
    "lists = sorted(important_features_dict.items(), key=lambda item: item[1],reverse=False)\n",
    "x,y = zip(*lists)\n",
    "\n",
    "\n",
    "print(f'15 most important features: {lists[::-1][0:15]}')\n",
    "\n",
    "# Graphically\n",
    "plt.figure(figsize=(13,18))\n",
    "plt.barh(x,y)\n",
    "plt.title('Feature Importance', fontsize=20)\n",
    "plt.ylabel('Attribute', fontsize=15)\n",
    "plt.xlabel('Importance', fontsize=15)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714204d",
   "metadata": {},
   "source": [
    "### $\\color{green}{\\textit{Using RandomForestClassifiers with class weights to select the features:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ba42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another try but with class_weight to check if we can improve the recall metric\n",
    "df_model = Bdata[features_prob].copy(deep=True)\n",
    "print(df_model.shape)\n",
    "\n",
    "y = df_model[\"class\"]\n",
    "x = df_model.loc[:,df_model.columns != \"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)\n",
    "\n",
    "data_results = []\n",
    "\n",
    "for weight in range(1,20):\n",
    "    forest = RandomForestClassifier(n_estimators = 100, max_features = 40, max_depth = 40, class_weight={0:1,1:weight}, random_state = 42)\n",
    "    forest.fit(X_train,y_train)\n",
    "    model = SelectFromModel(forest, prefit=True)\n",
    "    recall_ = recall_score(y_test, forest.predict(X_test), average='macro')\n",
    "    names = model.get_support()\n",
    "    data_results.append([weight, model.transform(X_train).shape[1], recall_.round(decimals=2), X_train.columns[names].values])\n",
    "\n",
    "    \n",
    "results = pd.DataFrame(data_results, columns=['weight','predictors','recall','predictor_names'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcbd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdb49ae",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{ A set of 10 features give a recall of 0.96 with Random Forest Classifier using a weight of 2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_10 = ['Attr6', 'Attr15', 'Attr20', 'Attr23', 'Attr27', 'Attr39',\n",
    "        'Attr41', 'Attr59', 'Attr61', 'Attr64','class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dc8dc",
   "metadata": {},
   "source": [
    "### $\\color{red}{\\textit{From RandomForest we are getting 10 best featrues 'Attr6', 'Attr15', 'Attr20', 'Attr23', 'Attr27',} \\\\\n",
    "\\textit{'Attr39','Attr41', 'Attr59', 'Attr61', 'Attr64', our further analysis will involve these features. We will use} \\\\ \\textit{hyperparameter tuning to see if we can improve the recall of our model.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c50204",
   "metadata": {},
   "source": [
    "## *4. Rigorous Evaluation of short-listed models*\n",
    "### $\\color{green}{\\textit{In this section we will try different Classifiers to observe which perform best and} \\\\ \\textit{then will do Hyperparamter tuning:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = Bdata[features_10].copy(deep=True)\n",
    "print(df_model.shape)\n",
    "\n",
    "y = df_model[\"class\"]\n",
    "x = df_model.loc[:,df_model.columns != \"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8993190",
   "metadata": {},
   "source": [
    "### $\\color{blue}{\\textit{Short Note on using Class Weights:}}$\n",
    "\n",
    "$\\color{blue}{\\textit{The data shows has heavily imbalanced classes, in which only 4% represents bankrupt against a 96% non-} bankrupt.\\\\ \\textit{In order to overcome the observed imbalance, weights were used in the classifier as a proxy for prioritise  good} \\\\ \\textit{predictions for the bankruptcy class. This approach allowed to positively turn the sensitivity of the classifier } \\\\ \\textit{towards the prediction of bankruptcy.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=skf,scoring='recall')\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \n",
    "          \"% recall score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a68bf7",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{Clearly RandomForestClassifier is giving the best result till now, LogisticRegression fails to} \\\\ \\textit{converge, we should try scaler with increasing the max_iter, so that it would converge.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression Hypertuning\n",
    "\n",
    "LR = make_pipeline(StandardScaler(),LogisticRegression(penalty='l2'))\n",
    "\n",
    "LRparam_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'logisticregression__max_iter': list(range(1000,5000,1000)),\n",
    "    'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'logisticregression__class_weight':[{0:1,1:2},{0:1,1:4},{0:1,1:6},{0:1,1:8},{0:1,1:10}]     \n",
    "}\n",
    "\n",
    "LR_search = GridSearchCV(LR, param_grid=LRparam_grid, refit = True,scoring='recall', verbose = 3, cv=2)\n",
    "\n",
    "# fitting the model for grid search \n",
    "LR_search.fit(trainX , trainY)\n",
    "LR_search.best_params_\n",
    "\n",
    "\n",
    "# summarize\n",
    "print('Mean Recall: %.3f' % LR_search.best_score_)\n",
    "print('Config: %s' % LR_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709089f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = make_pipeline(StandardScaler(),LogisticRegression(C=1,max_iter=4000,class_weight={0:1,1:20},penalty = 'l2'))\n",
    "logistic.fit(X_train, y_train)\n",
    "recall = recall_score(y_test,logistic.predict(X_test))\n",
    "f1 = f1_score(y_test,logistic.predict(X_test))\n",
    "recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logistic = logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f1f43",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{After Hyperparameter tuning of LogisticRegression we get the parameters :{'C'= 1, class_weight'= {0: 1, 1: 20}, 'max_iter'= 4000, 'solver': 'sag','penalty' = 'l2'} } \\\\ \\textit{and mean best score which we got was of recall:66.3% on KFold of 2.}}$ \n",
    "\n",
    "### $\\color{green}{\\textit{Lets try K-NN with Kfold of 5 using our best estimator:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6672e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knears_params = {\"n_neighbors\": list(range(2,8,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params,scoring='recall',cv=skf)\n",
    "grid_knears.fit(X_train, y_train)\n",
    "# KNears best estimator\n",
    "grid_KNN = grid_knears.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f6b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_knears.best_params_,grid_knears.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119821c",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{After Hyperparameter tuning of KNN we get the parameters :{'algorithm'= 1,'n_neighbors = 2},} \\\\ \\textit{and mean best score which we got was of recall:35.8%}}$\n",
    "\n",
    "### $\\color{green}{\\textit{SVC:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier GridSearch\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "svc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf'],'class_weight':[{0:1,1:10},{0:1,1:15},{0:1,1:20},{0:1,1:25}]}\n",
    "grid_svc = GridSearchCV(SVC(probability=True), svc_params,scoring = 'recall',cv=skf)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "\n",
    "# SVC best estimator\n",
    "grid_SVC = grid_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svc.best_params_,grid_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff3639",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{After Hyperparameter tuning of SVC we get the parameters :{'C'= 0.5,'kernal' = 'rbf','class_weight' = {0: 1, 1: 25} },} \\\\ \\textit{and mean best score which we got was of recall: 66.8%}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fd439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking the recall and F1 score on the basis of class_weight, since our data is quite imbalance improving the class_weight \n",
    "# should have some impact on scoring\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "results_LR = []\n",
    "for weight in range(10,100,5):\n",
    "    logistic = make_pipeline(StandardScaler(),LogisticRegression(C=1,max_iter=10000,class_weight={0:1,1:weight},penalty = 'l2'))\n",
    "    logistic.fit(X_train, y_train)\n",
    "    recall = recall_score(y_test,logistic.predict(X_test))\n",
    "    f1 = f1_score(y_test,logistic.predict(X_test))\n",
    "    results_LR.append([weight,recall,f1])\n",
    "    \n",
    "pd.DataFrame(results_LR,columns=[\"weights\",\"Recall\",\"F1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f43c39",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{As we push recall F1 reduces as shown below:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of Recall and F1 scores on the basis of class_weight\n",
    "\n",
    "results_LR = pd.DataFrame(results_LR,columns=[\"weights\",\"Recall\",\"F1\"])\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Recall/F1 vs class_weight -Logistic Regression', fontsize=18)\n",
    "plt.plot(results_LR['weights'], results_LR['Recall'], label='Variation of Recall_Scores')\n",
    "plt.plot(results_LR['weights'], results_LR['F1'], label='Variation of F1_Scores')  \n",
    "plt.axis([5, 100, 0, 1])\n",
    "plt.xlabel('class_weights', fontsize=16)\n",
    "plt.ylabel('Recall/F1 scores- Logistic Regression', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d536656",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{Logistic Regression recall improves drastically by increasing the weight of the class, this will} \\\\ \\textit{introduce bias in our model. This can been seen by F1 score which going down as recall is increasing.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the recall and F1 score on the basis of class_weight, since our data is quite imbalance improving the class_weight \n",
    "# should have some impact on scoring\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "results_SVC = []\n",
    "for weight in range(10,100,5):\n",
    "    svc = SVC(C=0.5 , class_weight={0:1,1:weight}, kernel='rbf')\n",
    "    svc.fit(X_train, y_train)\n",
    "    recall = recall_score(y_test,svc.predict(X_test))\n",
    "    f1 = f1_score(y_test,svc.predict(X_test))\n",
    "    results_SVC.append([weight,recall,f1])\n",
    "    \n",
    "pd.DataFrame(results_SVC,columns=[\"weights\",\"Recall\",\"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variation of Recall and F1 scores on the basis of class_weight\n",
    "\n",
    "results_SVC = pd.DataFrame(results_SVC,columns=[\"weights\",\"Recall\",\"F1\"])\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Recall/F1 vs class_weight- SVC', fontsize=18)\n",
    "plt.plot(results_SVC['weights'], results_SVC['Recall'], label='Recall_Scores')\n",
    "plt.plot(results_SVC['weights'], results_SVC['F1'], label='F1_Scores')  \n",
    "plt.axis([5, 100, 0, 1])\n",
    "plt.xlabel('class_weights', fontsize=16)\n",
    "plt.ylabel('Recall/F1 scores', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627ecca",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{Support Vector Classifiers recall improves drastically by increasing the weight of the class, this will} \\\\ \\textit{introduce biased in our model. This can been seen by F1 score which going down as recall is increasing.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f073d2e7",
   "metadata": {},
   "source": [
    "### $\\color{green}{\\text{RandomForestClassifier: GridSearchCV to find the best Hyperparameters and we use stratifiedKfold = 5} \\\\ \\text{for cross validation:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# RandomForestClassifier\n",
    "forest_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(5,40,5)),'n_estimators':list(range(50,200,50)),\n",
    "                 'class_weight':[{0:1,1:10},{0:1,1:15},{0:1,1:20},{0:1,1:25}]}\n",
    "grid_forest = GridSearchCV(RandomForestClassifier(), forest_params,cv=skf)\n",
    "grid_forest.fit(X_train, y_train)\n",
    "\n",
    "# tree best estimator\n",
    "forest_clf = grid_forest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d583bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_forest.best_params_,grid_forest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_test,forest_clf.predict(X_test))\n",
    "f1     = f1_score(y_test,forest_clf.predict(X_test))\n",
    "print('RandomForest Classifier Recall Score', round(recall * 100, 2).astype(str) + '%')\n",
    "\n",
    "print('RandomForest Classifier F1 Score', round(f1 * 100, 2).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e5945",
   "metadata": {},
   "source": [
    "$\\color{red}{\\textit{On the test set RandomForestClassifier with parameters :{'class_weight': {0: 1, 1: 20},'criterion': 'entropy','max_depth':10,\n",
    " 'n_estimators': 50}} \\\\ \\textit{is giving best result. Next we will plot confusion matrix to validate our finding.}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Generate the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test, forest_clf.predict(X_test))\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt=\".0f\")\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c867b",
   "metadata": {},
   "source": [
    "## ROC and Precision/Recall Curve\n",
    "\n",
    "1. *ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using    different probability thresholds.*\n",
    "2. *Precision-Recall curves summarize the trade-off between the true positive rate and the positive predicted value for a predictive model using different probability thresholds.*\n",
    "3. *ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b42615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "# LogisticRegression\n",
    "\n",
    "log_pred = grid_logistic.predict_proba(X_test)[:,1]\n",
    "knn_pred = grid_KNN.predict_proba(X_test)[:,1]\n",
    "svc_pred = grid_SVC.predict_proba(X_test)[:,1]\n",
    "forest_pred = forest_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "log_fpr,log_tpr,log_threshold  = roc_curve(y_test,log_pred)\n",
    "knn_fpr,knn_tpr,knn_threshold  = roc_curve(y_test,knn_pred)\n",
    "svc_fpr,svc_tpr,svc_threshold  = roc_curve(y_test,svc_pred)\n",
    "forest_fpr,forest_tpr,forest_threshold  = roc_curve(y_test,forest_pred)\n",
    "\n",
    "def ROC_curve_multi(log_fpr,log_tpr,knn_fpr,knn_tpr,svc_fpr,svc_tpr,forest_fpr,forest_tpr):\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.2f}'.format(roc_auc_score(y_test, log_pred)*100))\n",
    "    plt.plot(knn_fpr, knn_tpr, label='KNears Neighbors Classifier Score: {:.2f}'.format(roc_auc_score(y_test, knn_pred)*100))\n",
    "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.2f}'.format(roc_auc_score(y_test, svc_pred)*100))\n",
    "    plt.plot(forest_fpr, forest_tpr, label='RandomForrestClassifier Score: {:.2f}'.format(roc_auc_score(y_test, forest_pred)*100))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "ROC_curve_multi(log_fpr, log_tpr, knn_fpr, knn_tpr, svc_fpr, svc_tpr, forest_fpr, forest_tpr)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "log_pre,log_rec,log_threshold  = precision_recall_curve(y_test,log_pred)\n",
    "knn_pre,knn_rec,knn_threshold  = precision_recall_curve(y_test,knn_pred)\n",
    "svc_pre,svc_rec,svc_threshold  = precision_recall_curve(y_test,svc_pred)\n",
    "forest_pre,forest_rec,forest_threshold  = precision_recall_curve(y_test,forest_pred)\n",
    "\n",
    "\n",
    "def Precision_Recall_curve_multi(log_fpr,log_tpr,knn_fpr,knn_tpr,svc_fpr,svc_tpr,forest_fpr,forest_tpr):\n",
    "    \n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('Precision-Recall Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_rec,log_pre, label='Logistic Regression Classifier Recall Score: {:.2f}, F1 Score: {:.2f}'\n",
    "             .format(recall_score(y_test,grid_logistic.predict(X_test))*100,f1_score(y_test,grid_logistic.predict(X_test))*100))\n",
    "    plt.plot(knn_rec, knn_pre, label='KNears Neighbors Classifier Recall Score: {:.2f}, F1 Score: {:.2f}'\n",
    "             .format(recall_score(y_test,grid_KNN.predict(X_test))*100,f1_score(y_test,grid_KNN.predict(X_test))*100))\n",
    "    plt.plot(svc_rec, svc_pre, label='Support Vector Classifier Recall Score: {:.2f}, F1 Score: {:.2f}'\n",
    "             .format(recall_score(y_test,grid_SVC.predict(X_test))*100,f1_score(y_test,grid_SVC.predict(X_test))*100))\n",
    "    plt.plot(forest_rec, forest_pre, label='RandomForrestClassifier Recall Score: {:.2f}, F1 Score: {:.2f}'\n",
    "             .format(recall_score(y_test,forest_clf.predict(X_test))*100,f1_score(y_test,forest_clf.predict(X_test))*100))\n",
    "    \n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('Recall', fontsize=16)\n",
    "    plt.ylabel('Precision', fontsize=16)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "Precision_Recall_curve_multi(log_pre,log_rec,knn_pre,knn_rec,svc_pre,svc_rec,forest_pre,forest_rec)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37776d6d",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Next we will train our RandomForestClassifier on the whole dataset:}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b544264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = Bdata[features_10].copy(deep=True)\n",
    "\n",
    "y_all = df_model[\"class\"]\n",
    "x_all = df_model.loc[:,df_model.columns != \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3761ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier with parameters :\n",
    "#{'class_weight': {0: 1, 1: 20},'criterion': 'entropy','max_depth':10, 'n_estimators': 50}\n",
    "final_model_production = RandomForestClassifier(n_estimators = 50, max_depth = 10, criterion='entropy', class_weight={0: 1, 1: 20}, random_state = 42)\n",
    "final_model_production.fit(x_all,y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f4834d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f13b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d309a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
